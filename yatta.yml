dir: ./
papers:
  - authors:
      - name: M. Botvinick
      - name: D. G. T. Barrett
      - name: P. Battaglia
      - name: N. de Freitas
      - name: D. Kumaran
      - name: J. Z Leibo
      - name: T. Lillicrap
      - name: J. Modayil
      - name: S. Mohamed
      - name: N. C. Rabinowitz
      - name: D. J. Rezende
      - name: A. Santoro
      - name: T. Schaul
      - name: C. Summerfield
      - name: G. Wayne
      - name: T. Weber
      - name: D. Wierstra
      - name: S. Legg
      - name: D. Hassabis
    categories:
      - cs.AI
    files:
      - 1711.08378v1.pdf
    id: 'http://arxiv.org/abs/1711.08378v1'
    links:
      - href: 'http://arxiv.org/abs/1711.08378v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1711.08378v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1711.08378v1'
    published: 2017-11-22T16:35:29.000Z
    summary: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
    title: >-
      Building Machines that Learn and Think for Themselves: Commentary on Lake
      et al., Behavioral and Brain Sciences, 2017
    updated: 2017-11-22T16:35:29.000Z
    url: 'http://arxiv.org/abs/1711.08378v1'
    year: 2017
  - authors:
      - name: Théophane Weber
      - name: Sébastien Racanière
      - name: David P. Reichert
      - name: Lars Buesing
      - name: Arthur Guez
      - name: Danilo Jimenez Rezende
      - name: Adria Puigdomènech Badia
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Yujia Li
      - name: Razvan Pascanu
      - name: Peter Battaglia
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
      - stat.ML
    files:
      - 1707.06203v1.pdf
    id: 'http://arxiv.org/abs/1707.06203v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06203v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06203v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06203v1'
    published: 2017-07-19T17:12:56.000Z
    summary: >-
      We introduce Imagination-Augmented Agents (I2As), a novel architecture for
      deep reinforcement learning combining model-free and model-based aspects.
      In contrast to most existing model-based reinforcement learning and
      planning methods, which prescribe how a model should be used to arrive at
      a policy, I2As learn to interpret predictions from a learned environment
      model to construct implicit plans in arbitrary ways, by using the
      predictions as additional context in deep policy networks. I2As show
      improved data efficiency, performance, and robustness to model
      misspecification compared to several baselines.
    title: Imagination-Augmented Agents for Deep Reinforcement Learning
    updated: 2017-07-19T17:12:56.000Z
    url: 'http://arxiv.org/abs/1707.06203v1'
    year: 2017
  - authors:
      - name: Razvan Pascanu
      - name: Yujia Li
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Lars Buesing
      - name: Sebastien Racanière
      - name: David Reichert
      - name: Théophane Weber
      - name: Daan Wierstra
      - name: Peter Battaglia
    categories:
      - cs.AI
      - cs.LG
      - cs.NE
      - stat.ML
    files:
      - 1707.06170v1.pdf
    id: 'http://arxiv.org/abs/1707.06170v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06170v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06170v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06170v1'
    published: 2017-07-19T15:52:35.000Z
    summary: >-
      Conventional wisdom holds that model-based planning is a powerful approach
      to sequential decision-making. It is often very challenging in practice,
      however, because while a model can be used to evaluate a plan, it does not
      prescribe how to construct a plan. Here we introduce the
      "Imagination-based Planner", the first model-based, sequential
      decision-making agent that can learn to construct, evaluate, and execute
      plans. Before any action, it can perform a variable number of imagination
      steps, which involve proposing an imagined action and evaluating it with
      its model-based imagination. All imagined actions and outcomes are
      aggregated, iteratively, into a "plan context" which conditions future
      real and imagined actions. The agent can even decide how to imagine:
      testing out alternative imagined actions, chaining sequences of actions
      together, or building a more complex "imagination tree" by navigating
      flexibly among the previously imagined states using a learned policy. And
      our agent can learn to plan economically, jointly optimizing for external
      rewards and computational costs associated with using its imagination. We
      show that our architecture can learn to solve a challenging continuous
      control problem, and also learn elaborate planning strategies in a
      discrete maze-solving task. Our work opens a new direction toward learning
      the components of a model-based planning system and how to use them.
    title: Learning model-based planning from scratch
    updated: 2017-07-19T15:52:35.000Z
    url: 'http://arxiv.org/abs/1707.06170v1'
    year: 2017
  - authors:
      - name: M. Botvinick
      - name: D. G. T. Barrett
      - name: P. Battaglia
      - name: N. de Freitas
      - name: D. Kumaran
      - name: J. Z Leibo
      - name: T. Lillicrap
      - name: J. Modayil
      - name: S. Mohamed
      - name: N. C. Rabinowitz
      - name: D. J. Rezende
      - name: A. Santoro
      - name: T. Schaul
      - name: C. Summerfield
      - name: G. Wayne
      - name: T. Weber
      - name: D. Wierstra
      - name: S. Legg
      - name: D. Hassabis
    categories:
      - cs.AI
    files:
      - 1711.08378v1.pdf
    id: 'http://arxiv.org/abs/1711.08378v1'
    links:
      - href: 'http://arxiv.org/abs/1711.08378v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1711.08378v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1711.08378v1'
    published: 2017-11-22T16:35:29.000Z
    summary: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
    title: >-
      Building Machines that Learn and Think for Themselves: Commentary on Lake
      et al., Behavioral and Brain Sciences, 2017
    updated: 2017-11-22T16:35:29.000Z
    url: 'http://arxiv.org/abs/1711.08378v1'
    year: 2017
  - authors:
      - name: Théophane Weber
      - name: Sébastien Racanière
      - name: David P. Reichert
      - name: Lars Buesing
      - name: Arthur Guez
      - name: Danilo Jimenez Rezende
      - name: Adria Puigdomènech Badia
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Yujia Li
      - name: Razvan Pascanu
      - name: Peter Battaglia
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
      - stat.ML
    files:
      - 1707.06203v1.pdf
    id: 'http://arxiv.org/abs/1707.06203v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06203v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06203v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06203v1'
    published: 2017-07-19T17:12:56.000Z
    summary: >-
      We introduce Imagination-Augmented Agents (I2As), a novel architecture for
      deep reinforcement learning combining model-free and model-based aspects.
      In contrast to most existing model-based reinforcement learning and
      planning methods, which prescribe how a model should be used to arrive at
      a policy, I2As learn to interpret predictions from a learned environment
      model to construct implicit plans in arbitrary ways, by using the
      predictions as additional context in deep policy networks. I2As show
      improved data efficiency, performance, and robustness to model
      misspecification compared to several baselines.
    title: Imagination-Augmented Agents for Deep Reinforcement Learning
    updated: 2017-07-19T17:12:56.000Z
    url: 'http://arxiv.org/abs/1707.06203v1'
    year: 2017
  - authors:
      - name: Razvan Pascanu
      - name: Yujia Li
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Lars Buesing
      - name: Sebastien Racanière
      - name: David Reichert
      - name: Théophane Weber
      - name: Daan Wierstra
      - name: Peter Battaglia
    categories:
      - cs.AI
      - cs.LG
      - cs.NE
      - stat.ML
    files:
      - 1707.06170v1.pdf
    id: 'http://arxiv.org/abs/1707.06170v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06170v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06170v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06170v1'
    published: 2017-07-19T15:52:35.000Z
    summary: >-
      Conventional wisdom holds that model-based planning is a powerful approach
      to sequential decision-making. It is often very challenging in practice,
      however, because while a model can be used to evaluate a plan, it does not
      prescribe how to construct a plan. Here we introduce the
      "Imagination-based Planner", the first model-based, sequential
      decision-making agent that can learn to construct, evaluate, and execute
      plans. Before any action, it can perform a variable number of imagination
      steps, which involve proposing an imagined action and evaluating it with
      its model-based imagination. All imagined actions and outcomes are
      aggregated, iteratively, into a "plan context" which conditions future
      real and imagined actions. The agent can even decide how to imagine:
      testing out alternative imagined actions, chaining sequences of actions
      together, or building a more complex "imagination tree" by navigating
      flexibly among the previously imagined states using a learned policy. And
      our agent can learn to plan economically, jointly optimizing for external
      rewards and computational costs associated with using its imagination. We
      show that our architecture can learn to solve a challenging continuous
      control problem, and also learn elaborate planning strategies in a
      discrete maze-solving task. Our work opens a new direction toward learning
      the components of a model-based planning system and how to use them.
    title: Learning model-based planning from scratch
    updated: 2017-07-19T15:52:35.000Z
    url: 'http://arxiv.org/abs/1707.06170v1'
    year: 2017
  - authors:
      - name: Ivo Danihelka
      - name: Balaji Lakshminarayanan
      - name: Benigno Uria
      - name: Daan Wierstra
      - name: Peter Dayan
    categories:
      - cs.LG
    files:
      - 1705.05263v1.pdf
    id: 'http://arxiv.org/abs/1705.05263v1'
    links:
      - href: 'http://arxiv.org/abs/1705.05263v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1705.05263v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1705.05263v1'
    published: 2017-05-15T14:24:01.000Z
    summary: >-
      We train a generator by maximum likelihood and we also train the same
      generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein
      distances. We show that an independent critic trained to approximate
      Wasserstein distance between the validation set and the generator
      distribution helps detect overfitting. Finally, we use ideas from the
      one-shot learning literature to develop a novel fast learning critic.
    title: Comparison of Maximum Likelihood and GAN-based training of Real NVPs
    updated: 2017-05-15T14:24:01.000Z
    url: 'http://arxiv.org/abs/1705.05263v1'
    year: 2017
  - authors:
      - name: Silvia Chiappa
      - name: Sébastien Racaniere
      - name: Daan Wierstra
      - name: Shakir Mohamed
    categories:
      - cs.AI
      - cs.LG
      - stat.ML
    files:
      - 1704.02254v2.pdf
    id: 'http://arxiv.org/abs/1704.02254v2'
    links:
      - href: 'http://arxiv.org/abs/1704.02254v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1704.02254v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1704.02254v2'
    published: 2017-04-07T14:53:54.000Z
    summary: >-
      Models that can simulate how environments change in response to actions
      can be used by agents to plan and act efficiently. We improve on previous
      environment simulators from high-dimensional pixel observations by
      introducing recurrent neural networks that are able to make temporally and
      spatially coherent predictions for hundreds of time-steps into the future.
      We present an in-depth analysis of the factors affecting performance,
      providing the most extensive attempt to advance the understanding of the
      properties of these models. We address the issue of computationally
      inefficiency with a model that does not need to generate a
      high-dimensional image at each time-step. We show that our approach can be
      used to improve exploration and is adaptable to many diverse environments,
      namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.
    title: Recurrent Environment Simulators
    updated: 2017-04-19T15:43:32.000Z
    url: 'http://arxiv.org/abs/1704.02254v2'
    year: 2017
  - authors:
      - name: Alexander Pritzel
      - name: Benigno Uria
      - name: Sriram Srinivasan
      - name: Adrià Puigdomènech
      - name: Oriol Vinyals
      - name: Demis Hassabis
      - name: Daan Wierstra
      - name: Charles Blundell
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1703.01988v1.pdf
    id: 'http://arxiv.org/abs/1703.01988v1'
    links:
      - href: 'http://arxiv.org/abs/1703.01988v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1703.01988v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1703.01988v1'
    published: 2017-03-06T17:23:27.000Z
    summary: >-
      Deep reinforcement learning methods attain super-human performance in a
      wide range of environments. Such methods are grossly inefficient, often
      taking orders of magnitudes more data than humans to achieve reasonable
      performance. We propose Neural Episodic Control: a deep reinforcement
      learning agent that is able to rapidly assimilate new experiences and act
      upon them. Our agent uses a semi-tabular representation of the value
      function: a buffer of past experience containing slowly changing state
      representations and rapidly updated estimates of the value function. We
      show across a wide range of environments that our agent learns
      significantly faster than other state-of-the-art, general purpose deep
      reinforcement learning agents.
    title: Neural Episodic Control
    updated: 2017-03-06T17:23:27.000Z
    url: 'http://arxiv.org/abs/1703.01988v1'
    year: 2017
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Charles Blundell
      - name: Yori Zwols
      - name: David Ha
      - name: Andrei A. Rusu
      - name: Alexander Pritzel
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.LG
    files:
      - 1701.08734v1.pdf
    id: 'http://arxiv.org/abs/1701.08734v1'
    links:
      - href: 'http://arxiv.org/abs/1701.08734v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1701.08734v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1701.08734v1'
    published: 2017-01-30T18:06:07.000Z
    summary: >-
      For artificial general intelligence (AGI) it would be efficient if
      multiple users trained the same giant neural network, permitting parameter
      reuse, without catastrophic forgetting. PathNet is a first step in this
      direction. It is a neural network algorithm that uses agents embedded in
      the neural network whose task is to discover which parts of the network to
      re-use for new tasks. Agents are pathways (views) through the network
      which determine the subset of parameters that are used and updated by the
      forwards and backwards passes of the backpropogation algorithm. During
      learning, a tournament selection genetic algorithm is used to select
      pathways through the neural network for replication and mutation. Pathway
      fitness is the performance of that pathway measured according to a cost
      function. We demonstrate successful transfer learning; fixing the
      parameters along a path learned on task A and re-evolving a new population
      of paths for task B, allows task B to be learned faster than it could be
      learned from scratch or after fine-tuning. Paths evolved on task B re-use
      parts of the optimal path evolved on task A. Positive transfer was
      demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
      classification tasks, and a set of Atari and Labyrinth reinforcement
      learning tasks, suggesting PathNets have general applicability for neural
      network training. Finally, PathNet also significantly improves the
      robustness to hyperparameter choices of a parallel asynchronous
      reinforcement learning algorithm (A3C).
    title: 'PathNet: Evolution Channels Gradient Descent in Super Neural Networks'
    updated: 2017-01-30T18:06:07.000Z
    url: 'http://arxiv.org/abs/1701.08734v1'
    year: 2017
  - authors:
      - name: Karol Gregor
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
    files:
      - 1611.07507v1.pdf
    id: 'http://arxiv.org/abs/1611.07507v1'
    links:
      - href: 'http://arxiv.org/abs/1611.07507v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1611.07507v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1611.07507v1'
    published: 2016-11-22T20:44:39.000Z
    summary: >-
      In this paper we introduce a new unsupervised reinforcement learning
      method for discovering the set of intrinsic options available to an agent.
      This set is learned by maximizing the number of different states an agent
      can reliably reach, as measured by the mutual information between the set
      of options and option termination states. To this end, we instantiate two
      policy gradient based algorithms, one that creates an explicit embedding
      space of options and one that represents options implicitly. The
      algorithms also provide an explicit measure of empowerment in a given
      state that can be used by an empowerment maximizing agent. The algorithm
      scales well with function approximation and we demonstrate the
      applicability of the algorithm on a range of tasks.
    title: Variational Intrinsic Control
    updated: 2016-11-22T20:44:39.000Z
    url: 'http://arxiv.org/abs/1611.07507v1'
    year: 2016
  - authors:
      - name: Charles Blundell
      - name: Benigno Uria
      - name: Alexander Pritzel
      - name: Yazhe Li
      - name: Avraham Ruderman
      - name: Joel Z Leibo
      - name: Jack Rae
      - name: Daan Wierstra
      - name: Demis Hassabis
    categories:
      - stat.ML
      - cs.LG
      - q-bio.NC
    files:
      - 1606.04460v1.pdf
    id: 'http://arxiv.org/abs/1606.04460v1'
    links:
      - href: 'http://arxiv.org/abs/1606.04460v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04460v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04460v1'
    published: 2016-06-14T17:03:46.000Z
    summary: >-
      State of the art deep reinforcement learning algorithms take many millions
      of interactions to attain human-level performance. Humans, on the other
      hand, can very quickly exploit highly rewarding nuances of an environment
      upon first discovery. In the brain, such rapid learning is thought to
      depend on the hippocampus and its capacity for episodic memory. Here we
      investigate whether a simple model of hippocampal episodic control can
      learn to solve difficult sequential decision-making tasks. We demonstrate
      that it not only attains a highly rewarding strategy significantly faster
      than state-of-the-art deep reinforcement learning algorithms, but also
      achieves a higher overall reward on some of the more challenging domains.
    title: Model-Free Episodic Control
    updated: 2016-06-14T17:03:46.000Z
    url: 'http://arxiv.org/abs/1606.04460v1'
    year: 2016
  - authors:
      - name: Oriol Vinyals
      - name: Charles Blundell
      - name: Timothy Lillicrap
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1606.04080v2.pdf
    id: 'http://arxiv.org/abs/1606.04080v2'
    links:
      - href: 'http://arxiv.org/abs/1606.04080v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04080v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04080v2'
    published: 2016-06-13T19:34:22.000Z
    summary: >-
      Learning from a few examples remains a key challenge in machine learning.
      Despite recent advances in important domains such as vision and language,
      the standard supervised deep learning paradigm does not offer a
      satisfactory solution for learning new concepts rapidly from little data.
      In this work, we employ ideas from metric learning based on deep neural
      features and from recent advances that augment neural networks with
      external memories. Our framework learns a network that maps a small
      labelled support set and an unlabelled example to its label, obviating the
      need for fine-tuning to adapt to new class types. We then define one-shot
      learning problems on vision (using Omniglot, ImageNet) and language tasks.
      Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2%
      and from 88.0% to 93.8% on Omniglot compared to competing approaches. We
      also demonstrate the usefulness of the same model on language modeling by
      introducing a one-shot task on the Penn Treebank.
    title: Matching Networks for One Shot Learning
    updated: 2017-12-29T17:45:19.000Z
    url: 'http://arxiv.org/abs/1606.04080v2'
    year: 2016
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Malcolm Reynolds
      - name: Frederic Besse
      - name: David Pfau
      - name: Max Jaderberg
      - name: Marc Lanctot
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.CV
      - cs.LG
    files:
      - 1606.02580v1.pdf
    id: 'http://arxiv.org/abs/1606.02580v1'
    links:
      - href: 'http://arxiv.org/abs/1606.02580v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.02580v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.02580v1'
    published: 2016-06-08T14:37:39.000Z
    summary: >-
      In this work we introduce a differentiable version of the Compositional
      Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the
      topology of a DPPN is evolved but the weights are learned. A Lamarckian
      algorithm, that combines evolution and learning, produces DPPNs to
      reconstruct an image. Our main result is that DPPNs can be evolved/trained
      to compress the weights of a denoising autoencoder from 157684 to roughly
      200 parameters, while achieving a reconstruction accuracy comparable to a
      fully connected network with more than two orders of magnitude more
      parameters. The regularization ability of the DPPN allows it to rediscover
      (approximate) convolutional network architectures embedded within a fully
      connected architecture. Such convolutional architectures are the current
      state of the art for many computer vision applications, so it is
      satisfying that DPPNs are capable of discovering this structure rather
      than having to build it in by design. DPPNs exhibit better generalization
      when tested on the Omniglot dataset after being trained on MNIST, than
      directly encoded fully connected autoencoders. DPPNs are therefore a new
      framework for integrating learning and evolution.
    title: 'Convolution by Evolution: Differentiable Pattern Producing Networks'
    updated: 2016-06-08T14:37:39.000Z
    url: 'http://arxiv.org/abs/1606.02580v1'
    year: 2016
  - authors:
      - name: Adam Santoro
      - name: Sergey Bartunov
      - name: Matthew Botvinick
      - name: Daan Wierstra
      - name: Timothy Lillicrap
    categories:
      - cs.LG
    files:
      - 1605.06065v1.pdf
    id: 'http://arxiv.org/abs/1605.06065v1'
    links:
      - href: 'http://arxiv.org/abs/1605.06065v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1605.06065v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1605.06065v1'
    published: 2016-05-19T17:44:51.000Z
    summary: >-
      Despite recent breakthroughs in the applications of deep neural networks,
      one setting that presents a persistent challenge is that of "one-shot
      learning." Traditional gradient-based networks require a lot of data to
      learn, often through extensive iterative training. When new data is
      encountered, the models must inefficiently relearn their parameters to
      adequately incorporate the new information without catastrophic
      interference. Architectures with augmented memory capacities, such as
      Neural Turing Machines (NTMs), offer the ability to quickly encode and
      retrieve new information, and hence can potentially obviate the downsides
      of conventional models. Here, we demonstrate the ability of a
      memory-augmented neural network to rapidly assimilate new data, and
      leverage this data to make accurate predictions after only a few samples.
      We also introduce a new method for accessing an external memory that
      focuses on memory content, unlike previous methods that additionally use
      memory location-based focusing mechanisms.
    title: One-shot Learning with Memory-Augmented Neural Networks
    updated: 2016-05-19T17:44:51.000Z
    url: 'http://arxiv.org/abs/1605.06065v1'
    year: 2016
  - authors:
      - name: Karol Gregor
      - name: Frederic Besse
      - name: Danilo Jimenez Rezende
      - name: Ivo Danihelka
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.CV
      - cs.LG
    files:
      - 1604.08772v1.pdf
    id: 'http://arxiv.org/abs/1604.08772v1'
    links:
      - href: 'http://arxiv.org/abs/1604.08772v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1604.08772v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1604.08772v1'
    published: 2016-04-29T11:02:52.000Z
    summary: >-
      We introduce a simple recurrent variational auto-encoder architecture that
      significantly improves image modeling. The system represents the
      state-of-the-art in latent variable models for both the ImageNet and
      Omniglot datasets. We show that it naturally separates global conceptual
      information from lower level details, thus addressing one of the
      fundamentally desired properties of unsupervised learning. Furthermore,
      the possibility of restricting ourselves to storing only global
      information about an image allows us to achieve high quality 'conceptual
      compression'.
    title: Towards Conceptual Compression
    updated: 2016-04-29T11:02:52.000Z
    url: 'http://arxiv.org/abs/1604.08772v1'
    year: 2016
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Ivo Danihelka
      - name: Karol Gregor
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
    files:
      - 1603.05106v2.pdf
    id: 'http://arxiv.org/abs/1603.05106v2'
    links:
      - href: 'http://arxiv.org/abs/1603.05106v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1603.05106v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1603.05106v2'
    published: 2016-03-16T14:10:00.000Z
    summary: >-
      Humans have an impressive ability to reason about new concepts and
      experiences from just a single example. In particular, humans have an
      ability for one-shot generalization: an ability to encounter a new
      concept, understand its structure, and then be able to generate compelling
      alternative variations of the concept. We develop machine learning systems
      with this important capacity by developing new deep generative models,
      models that combine the representational power of deep learning with the
      inferential power of Bayesian reasoning. We develop a class of sequential
      generative models that are built on the principles of feedback and
      attention. These two characteristics lead to generative models that are
      among the state-of-the art in density estimation and image generation. We
      demonstrate the one-shot generalization ability of our models using three
      tasks: unconditional sampling, generating new exemplars of a given
      concept, and generating new exemplars of a family of concepts. In all
      cases our models are able to generate compelling and diverse
      samples---having seen new examples just once---providing an important
      class of general-purpose models for one-shot machine learning.
    title: One-Shot Generalization in Deep Generative Models
    updated: 2016-05-25T12:57:19.000Z
    url: 'http://arxiv.org/abs/1603.05106v2'
    year: 2016
  - authors:
      - name: Timothy P. Lillicrap
      - name: Jonathan J. Hunt
      - name: Alexander Pritzel
      - name: Nicolas Heess
      - name: Tom Erez
      - name: Yuval Tassa
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1509.02971v5.pdf
    id: 'http://arxiv.org/abs/1509.02971v5'
    links:
      - href: 'http://arxiv.org/abs/1509.02971v5'
        title: ''
      - href: 'http://arxiv.org/pdf/1509.02971v5'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1509.02971v5'
    published: 2015-09-09T23:01:36.000Z
    summary: >-
      We adapt the ideas underlying the success of Deep Q-Learning to the
      continuous action domain. We present an actor-critic, model-free algorithm
      based on the deterministic policy gradient that can operate over
      continuous action spaces. Using the same learning algorithm, network
      architecture and hyper-parameters, our algorithm robustly solves more than
      20 simulated physics tasks, including classic problems such as cartpole
      swing-up, dexterous manipulation, legged locomotion and car driving. Our
      algorithm is able to find policies whose performance is competitive with
      those found by a planning algorithm with full access to the dynamics of
      the domain and its derivatives. We further demonstrate that for many of
      the tasks the algorithm can learn policies end-to-end: directly from raw
      pixel inputs.
    title: Continuous control with deep reinforcement learning
    updated: 2016-02-29T18:45:53.000Z
    url: 'http://arxiv.org/abs/1509.02971v5'
    year: 2015
  - authors:
      - name: Charles Blundell
      - name: Julien Cornebise
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.LG
    files:
      - 1505.05424v2.pdf
    id: 'http://arxiv.org/abs/1505.05424v2'
    links:
      - href: 'http://arxiv.org/abs/1505.05424v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1505.05424v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1505.05424v2'
    published: 2015-05-20T15:39:48.000Z
    summary: >-
      We introduce a new, efficient, principled and backpropagation-compatible
      algorithm for learning a probability distribution on the weights of a
      neural network, called Bayes by Backprop. It regularises the weights by
      minimising a compression cost, known as the variational free energy or the
      expected lower bound on the marginal likelihood. We show that this
      principled kind of regularisation yields comparable performance to dropout
      on MNIST classification. We then demonstrate how the learnt uncertainty in
      the weights can be used to improve generalisation in non-linear regression
      problems, and how this weight uncertainty can be used to drive the
      exploration-exploitation trade-off in reinforcement learning.
    title: Weight Uncertainty in Neural Networks
    updated: 2015-05-21T14:07:23.000Z
    url: 'http://arxiv.org/abs/1505.05424v2'
    year: 2015
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Alex Graves
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.CV
      - cs.LG
      - cs.NE
    files:
      - 1502.04623v2.pdf
    id: 'http://arxiv.org/abs/1502.04623v2'
    links:
      - href: 'http://arxiv.org/abs/1502.04623v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1502.04623v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1502.04623v2'
    published: 2015-02-16T16:48:56.000Z
    summary: >-
      This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
      network architecture for image generation. DRAW networks combine a novel
      spatial attention mechanism that mimics the foveation of the human eye,
      with a sequential variational auto-encoding framework that allows for the
      iterative construction of complex images. The system substantially
      improves on the state of the art for generative models on MNIST, and, when
      trained on the Street View House Numbers dataset, it generates images that
      cannot be distinguished from real data with the naked eye.
    title: 'DRAW: A Recurrent Neural Network For Image Generation'
    updated: 2015-05-20T15:29:42.000Z
    url: 'http://arxiv.org/abs/1502.04623v2'
    year: 2015
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
      - stat.CO
      - stat.ME
    files:
      - 1401.4082v3.pdf
    id: 'http://arxiv.org/abs/1401.4082v3'
    links:
      - href: 'http://arxiv.org/abs/1401.4082v3'
        title: ''
      - href: 'http://arxiv.org/pdf/1401.4082v3'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1401.4082v3'
    published: 2014-01-16T16:33:23.000Z
    summary: >-
      We marry ideas from deep neural networks and approximate Bayesian
      inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning.
      Our algorithm introduces a recognition model to represent approximate
      posterior distributions, and that acts as a stochastic encoder of the
      data. We develop stochastic back-propagation -- rules for back-propagation
      through stochastic variables -- and use this to develop an algorithm that
      allows for joint optimisation of the parameters of both the generative and
      recognition model. We demonstrate on several real-world data sets that the
      model generates realistic samples, provides accurate imputations of
      missing data and is a useful tool for high-dimensional data visualisation.
    title: >-
      Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models
    updated: 2014-05-30T10:00:36.000Z
    url: 'http://arxiv.org/abs/1401.4082v3'
    year: 2014
  - authors:
      - name: Volodymyr Mnih
      - name: Koray Kavukcuoglu
      - name: David Silver
      - name: Alex Graves
      - name: Ioannis Antonoglou
      - name: Daan Wierstra
      - name: Martin Riedmiller
    categories:
      - cs.LG
    files:
      - 1312.5602v1.pdf
    id: 'http://arxiv.org/abs/1312.5602v1'
    links:
      - href: 'http://arxiv.org/abs/1312.5602v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1312.5602v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1312.5602v1'
    published: 2013-12-19T16:00:08.000Z
    summary: >-
      We present the first deep learning model to successfully learn control
      policies directly from high-dimensional sensory input using reinforcement
      learning. The model is a convolutional neural network, trained with a
      variant of Q-learning, whose input is raw pixels and whose output is a
      value function estimating future rewards. We apply our method to seven
      Atari 2600 games from the Arcade Learning Environment, with no adjustment
      of the architecture or learning algorithm. We find that it outperforms all
      previous approaches on six of the games and surpasses a human expert on
      three of them.
    title: Playing Atari with Deep Reinforcement Learning
    updated: 2013-12-19T16:00:08.000Z
    url: 'http://arxiv.org/abs/1312.5602v1'
    year: 2013
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Andriy Mnih
      - name: Charles Blundell
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1310.8499v2.pdf
    id: 'http://arxiv.org/abs/1310.8499v2'
    links:
      - href: 'http://arxiv.org/abs/1310.8499v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1310.8499v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1310.8499v2'
    published: 2013-10-31T13:47:30.000Z
    summary: >-
      We introduce a deep, generative autoencoder capable of learning
      hierarchies of distributed representations from data. Successive deep
      stochastic hidden layers are equipped with autoregressive connections,
      which enable the model to be sampled from quickly and exactly via
      ancestral sampling. We derive an efficient approximate parameter
      estimation method based on the minimum description length (MDL) principle,
      which can be seen as maximising a variational lower bound on the
      log-likelihood, with a feedforward neural network implementing approximate
      inference. We demonstrate state-of-the-art generative performance on a
      number of classic data sets: several UCI data sets, MNIST and Atari 2600
      games.
    title: Deep AutoRegressive Networks
    updated: 2014-05-20T16:22:43.000Z
    url: 'http://arxiv.org/abs/1310.8499v2'
    year: 2013
  - authors:
      - name: Yi Sun
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Juergen Schmidhuber
    categories:
      - cs.AI
    files:
      - 1209.5853v1.pdf
    id: 'http://arxiv.org/abs/1209.5853v1'
    links:
      - href: 'http://arxiv.org/abs/1209.5853v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1209.5853v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1209.5853v1'
    published: 2012-09-26T07:42:06.000Z
    summary: >-
      Efficient Natural Evolution Strategies (eNES) is a novel alternative to
      conventional evolutionary algorithms, using the natural gradient to adapt
      the mutation distribution. Unlike previous methods based on natural
      gradients, eNES uses a fast algorithm to calculate the inverse of the
      exact Fisher information matrix, thus increasing both robustness and
      performance of its evolution gradient estimation, even in higher
      dimensions. Additional novel aspects of eNES include optimal fitness
      baselines and importance mixing (a procedure for updating the population
      with very few fitness evaluations). The algorithm yields competitive
      results on both unimodal and multimodal benchmarks.
    title: Efficient Natural Evolution Strategies
    updated: 2012-09-26T07:42:06.000Z
    url: 'http://arxiv.org/abs/1209.5853v1'
    year: 2012
  - authors:
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Tobias Glasmachers
      - name: Yi Sun
      - name: Jürgen Schmidhuber
    categories:
      - stat.ML
      - cs.NE
    files:
      - 1106.4487v1.pdf
    id: 'http://arxiv.org/abs/1106.4487v1'
    links:
      - href: 'http://arxiv.org/abs/1106.4487v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1106.4487v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1106.4487v1'
    published: 2011-06-22T15:55:52.000Z
    summary: >-
      This paper presents Natural Evolution Strategies (NES), a recent family of
      algorithms that constitute a more principled approach to black-box
      optimization than established evolutionary algorithms. NES maintains a
      parameterized distribution on the set of solution candidates, and the
      natural gradient is used to update the distribution's parameters in the
      direction of higher expected fitness. We introduce a collection of
      techniques that address issues of convergence, robustness, sample
      complexity, computational complexity and sensitivity to hyperparameters.
      This paper explores a number of implementations of the NES family, ranging
      from general-purpose multi-variate normal distributions to heavy-tailed
      and separable distributions tailored towards global optimization and
      search in high dimensional spaces, respectively. Experimental results show
      best published performance on various standard benchmarks, as well as
      competitive performance on others.
    title: Natural Evolution Strategies
    updated: 2011-06-22T15:55:52.000Z
    url: 'http://arxiv.org/abs/1106.4487v1'
    year: 2011
  - authors:
      - name: Juergen Schmidhuber
      - name: Matteo Gagliolo
      - name: Daan Wierstra
      - name: Faustino Gomez
    categories:
      - cs.NE
      - F.1.1; I.2.6
    files:
      - 0512062v1.pdf
    id: 'http://arxiv.org/abs/cs/0512062v1'
    links:
      - href: 'http://arxiv.org/abs/cs/0512062v1'
        title: ''
      - href: 'http://arxiv.org/pdf/cs/0512062v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/cs/0512062v1'
    published: 2005-12-15T15:05:22.000Z
    summary: >-
      Traditional Support Vector Machines (SVMs) need pre-wired finite time
      windows to predict and classify time series. They do not have an internal
      state necessary to deal with sequences involving arbitrary long-term
      dependencies. Here we introduce a new class of recurrent, truly sequential
      SVM-like devices with internal adaptive states, trained by a novel method
      called EVOlution of systems with KErnel-based outputs (Evoke), an instance
      of the recent Evolino class of methods. Evoke evolves recurrent neural
      networks to detect and represent temporal dependencies while using
      quadratic programming/support vector regression to produce precise
      outputs. Evoke is the first SVM-based mechanism learning to classify a
      context-sensitive language. It also outperforms recent state-of-the-art
      gradient-based recurrent neural networks (RNNs) on various time series
      prediction tasks.
    title: Evolino for recurrent support vector machines
    updated: 2005-12-15T15:05:22.000Z
    url: 'http://arxiv.org/abs/cs/0512062v1'
    year: 2005
  - authors:
      - name: Juergen Schmidhuber
      - name: Matteo Gagliolo
      - name: Daan Wierstra
      - name: Faustino Gomez
    categories:
      - cs.NE
      - F.1.1; I.2.6
    files:
      - 0512062v1.pdf
    id: 'http://arxiv.org/abs/cs/0512062v1'
    links:
      - href: 'http://arxiv.org/abs/cs/0512062v1'
        title: ''
      - href: 'http://arxiv.org/pdf/cs/0512062v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/cs/0512062v1'
    published: 2005-12-15T15:05:22.000Z
    summary: >-
      Traditional Support Vector Machines (SVMs) need pre-wired finite time
      windows to predict and classify time series. They do not have an internal
      state necessary to deal with sequences involving arbitrary long-term
      dependencies. Here we introduce a new class of recurrent, truly sequential
      SVM-like devices with internal adaptive states, trained by a novel method
      called EVOlution of systems with KErnel-based outputs (Evoke), an instance
      of the recent Evolino class of methods. Evoke evolves recurrent neural
      networks to detect and represent temporal dependencies while using
      quadratic programming/support vector regression to produce precise
      outputs. Evoke is the first SVM-based mechanism learning to classify a
      context-sensitive language. It also outperforms recent state-of-the-art
      gradient-based recurrent neural networks (RNNs) on various time series
      prediction tasks.
    title: Evolino for recurrent support vector machines
    updated: 2005-12-15T15:05:22.000Z
    url: 'http://arxiv.org/abs/cs/0512062v1'
    year: 2005
  - authors:
      - name: Charles Blundell
      - name: Julien Cornebise
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.LG
    files:
      - 1505.05424v2.pdf
    id: 'http://arxiv.org/abs/1505.05424v2'
    links:
      - href: 'http://arxiv.org/abs/1505.05424v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1505.05424v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1505.05424v2'
    published: 2015-05-20T15:39:48.000Z
    summary: >-
      We introduce a new, efficient, principled and backpropagation-compatible
      algorithm for learning a probability distribution on the weights of a
      neural network, called Bayes by Backprop. It regularises the weights by
      minimising a compression cost, known as the variational free energy or the
      expected lower bound on the marginal likelihood. We show that this
      principled kind of regularisation yields comparable performance to dropout
      on MNIST classification. We then demonstrate how the learnt uncertainty in
      the weights can be used to improve generalisation in non-linear regression
      problems, and how this weight uncertainty can be used to drive the
      exploration-exploitation trade-off in reinforcement learning.
    title: Weight Uncertainty in Neural Networks
    updated: 2015-05-21T14:07:23.000Z
    url: 'http://arxiv.org/abs/1505.05424v2'
    year: 2015
  - authors:
      - name: Théophane Weber
      - name: Sébastien Racanière
      - name: David P. Reichert
      - name: Lars Buesing
      - name: Arthur Guez
      - name: Danilo Jimenez Rezende
      - name: Adria Puigdomènech Badia
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Yujia Li
      - name: Razvan Pascanu
      - name: Peter Battaglia
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
      - stat.ML
    files:
      - 1707.06203v1.pdf
    id: 'http://arxiv.org/abs/1707.06203v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06203v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06203v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06203v1'
    published: 2017-07-19T17:12:56.000Z
    summary: >-
      We introduce Imagination-Augmented Agents (I2As), a novel architecture for
      deep reinforcement learning combining model-free and model-based aspects.
      In contrast to most existing model-based reinforcement learning and
      planning methods, which prescribe how a model should be used to arrive at
      a policy, I2As learn to interpret predictions from a learned environment
      model to construct implicit plans in arbitrary ways, by using the
      predictions as additional context in deep policy networks. I2As show
      improved data efficiency, performance, and robustness to model
      misspecification compared to several baselines.
    title: Imagination-Augmented Agents for Deep Reinforcement Learning
    updated: 2017-07-19T17:12:56.000Z
    url: 'http://arxiv.org/abs/1707.06203v1'
    year: 2017
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Andriy Mnih
      - name: Charles Blundell
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1310.8499v2.pdf
    id: 'http://arxiv.org/abs/1310.8499v2'
    links:
      - href: 'http://arxiv.org/abs/1310.8499v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1310.8499v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1310.8499v2'
    published: 2013-10-31T13:47:30.000Z
    summary: >-
      We introduce a deep, generative autoencoder capable of learning
      hierarchies of distributed representations from data. Successive deep
      stochastic hidden layers are equipped with autoregressive connections,
      which enable the model to be sampled from quickly and exactly via
      ancestral sampling. We derive an efficient approximate parameter
      estimation method based on the minimum description length (MDL) principle,
      which can be seen as maximising a variational lower bound on the
      log-likelihood, with a feedforward neural network implementing approximate
      inference. We demonstrate state-of-the-art generative performance on a
      number of classic data sets: several UCI data sets, MNIST and Atari 2600
      games.
    title: Deep AutoRegressive Networks
    updated: 2014-05-20T16:22:43.000Z
    url: 'http://arxiv.org/abs/1310.8499v2'
    year: 2013
  - authors:
      - name: Charles Blundell
      - name: Benigno Uria
      - name: Alexander Pritzel
      - name: Yazhe Li
      - name: Avraham Ruderman
      - name: Joel Z Leibo
      - name: Jack Rae
      - name: Daan Wierstra
      - name: Demis Hassabis
    categories:
      - stat.ML
      - cs.LG
      - q-bio.NC
    files:
      - 1606.04460v1.pdf
    id: 'http://arxiv.org/abs/1606.04460v1'
    links:
      - href: 'http://arxiv.org/abs/1606.04460v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04460v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04460v1'
    published: 2016-06-14T17:03:46.000Z
    summary: >-
      State of the art deep reinforcement learning algorithms take many millions
      of interactions to attain human-level performance. Humans, on the other
      hand, can very quickly exploit highly rewarding nuances of an environment
      upon first discovery. In the brain, such rapid learning is thought to
      depend on the hippocampus and its capacity for episodic memory. Here we
      investigate whether a simple model of hippocampal episodic control can
      learn to solve difficult sequential decision-making tasks. We demonstrate
      that it not only attains a highly rewarding strategy significantly faster
      than state-of-the-art deep reinforcement learning algorithms, but also
      achieves a higher overall reward on some of the more challenging domains.
    title: Model-Free Episodic Control
    updated: 2016-06-14T17:03:46.000Z
    url: 'http://arxiv.org/abs/1606.04460v1'
    year: 2016
  - authors:
      - name: Volodymyr Mnih
      - name: Koray Kavukcuoglu
      - name: David Silver
      - name: Alex Graves
      - name: Ioannis Antonoglou
      - name: Daan Wierstra
      - name: Martin Riedmiller
    categories:
      - cs.LG
    files:
      - 1312.5602v1.pdf
    id: 'http://arxiv.org/abs/1312.5602v1'
    links:
      - href: 'http://arxiv.org/abs/1312.5602v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1312.5602v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1312.5602v1'
    published: 2013-12-19T16:00:08.000Z
    summary: >-
      We present the first deep learning model to successfully learn control
      policies directly from high-dimensional sensory input using reinforcement
      learning. The model is a convolutional neural network, trained with a
      variant of Q-learning, whose input is raw pixels and whose output is a
      value function estimating future rewards. We apply our method to seven
      Atari 2600 games from the Arcade Learning Environment, with no adjustment
      of the architecture or learning algorithm. We find that it outperforms all
      previous approaches on six of the games and surpasses a human expert on
      three of them.
    title: Playing Atari with Deep Reinforcement Learning
    updated: 2013-12-19T16:00:08.000Z
    url: 'http://arxiv.org/abs/1312.5602v1'
    year: 2013
  - authors:
      - name: Razvan Pascanu
      - name: Yujia Li
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Lars Buesing
      - name: Sebastien Racanière
      - name: David Reichert
      - name: Théophane Weber
      - name: Daan Wierstra
      - name: Peter Battaglia
    categories:
      - cs.AI
      - cs.LG
      - cs.NE
      - stat.ML
    files:
      - 1707.06170v1.pdf
    id: 'http://arxiv.org/abs/1707.06170v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06170v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06170v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06170v1'
    published: 2017-07-19T15:52:35.000Z
    summary: >-
      Conventional wisdom holds that model-based planning is a powerful approach
      to sequential decision-making. It is often very challenging in practice,
      however, because while a model can be used to evaluate a plan, it does not
      prescribe how to construct a plan. Here we introduce the
      "Imagination-based Planner", the first model-based, sequential
      decision-making agent that can learn to construct, evaluate, and execute
      plans. Before any action, it can perform a variable number of imagination
      steps, which involve proposing an imagined action and evaluating it with
      its model-based imagination. All imagined actions and outcomes are
      aggregated, iteratively, into a "plan context" which conditions future
      real and imagined actions. The agent can even decide how to imagine:
      testing out alternative imagined actions, chaining sequences of actions
      together, or building a more complex "imagination tree" by navigating
      flexibly among the previously imagined states using a learned policy. And
      our agent can learn to plan economically, jointly optimizing for external
      rewards and computational costs associated with using its imagination. We
      show that our architecture can learn to solve a challenging continuous
      control problem, and also learn elaborate planning strategies in a
      discrete maze-solving task. Our work opens a new direction toward learning
      the components of a model-based planning system and how to use them.
    title: Learning model-based planning from scratch
    updated: 2017-07-19T15:52:35.000Z
    url: 'http://arxiv.org/abs/1707.06170v1'
    year: 2017
  - authors:
      - name: Timothy P. Lillicrap
      - name: Jonathan J. Hunt
      - name: Alexander Pritzel
      - name: Nicolas Heess
      - name: Tom Erez
      - name: Yuval Tassa
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1509.02971v5.pdf
    id: 'http://arxiv.org/abs/1509.02971v5'
    links:
      - href: 'http://arxiv.org/abs/1509.02971v5'
        title: ''
      - href: 'http://arxiv.org/pdf/1509.02971v5'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1509.02971v5'
    published: 2015-09-09T23:01:36.000Z
    summary: >-
      We adapt the ideas underlying the success of Deep Q-Learning to the
      continuous action domain. We present an actor-critic, model-free algorithm
      based on the deterministic policy gradient that can operate over
      continuous action spaces. Using the same learning algorithm, network
      architecture and hyper-parameters, our algorithm robustly solves more than
      20 simulated physics tasks, including classic problems such as cartpole
      swing-up, dexterous manipulation, legged locomotion and car driving. Our
      algorithm is able to find policies whose performance is competitive with
      those found by a planning algorithm with full access to the dynamics of
      the domain and its derivatives. We further demonstrate that for many of
      the tasks the algorithm can learn policies end-to-end: directly from raw
      pixel inputs.
    title: Continuous control with deep reinforcement learning
    updated: 2016-02-29T18:45:53.000Z
    url: 'http://arxiv.org/abs/1509.02971v5'
    year: 2015
  - authors:
      - name: Yi Sun
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Juergen Schmidhuber
    categories:
      - cs.AI
    files:
      - 1209.5853v1.pdf
    id: 'http://arxiv.org/abs/1209.5853v1'
    links:
      - href: 'http://arxiv.org/abs/1209.5853v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1209.5853v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1209.5853v1'
    published: 2012-09-26T07:42:06.000Z
    summary: >-
      Efficient Natural Evolution Strategies (eNES) is a novel alternative to
      conventional evolutionary algorithms, using the natural gradient to adapt
      the mutation distribution. Unlike previous methods based on natural
      gradients, eNES uses a fast algorithm to calculate the inverse of the
      exact Fisher information matrix, thus increasing both robustness and
      performance of its evolution gradient estimation, even in higher
      dimensions. Additional novel aspects of eNES include optimal fitness
      baselines and importance mixing (a procedure for updating the population
      with very few fitness evaluations). The algorithm yields competitive
      results on both unimodal and multimodal benchmarks.
    title: Efficient Natural Evolution Strategies
    updated: 2012-09-26T07:42:06.000Z
    url: 'http://arxiv.org/abs/1209.5853v1'
    year: 2012
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Malcolm Reynolds
      - name: Frederic Besse
      - name: David Pfau
      - name: Max Jaderberg
      - name: Marc Lanctot
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.CV
      - cs.LG
    files:
      - 1606.02580v1.pdf
    id: 'http://arxiv.org/abs/1606.02580v1'
    links:
      - href: 'http://arxiv.org/abs/1606.02580v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.02580v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.02580v1'
    published: 2016-06-08T14:37:39.000Z
    summary: >-
      In this work we introduce a differentiable version of the Compositional
      Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the
      topology of a DPPN is evolved but the weights are learned. A Lamarckian
      algorithm, that combines evolution and learning, produces DPPNs to
      reconstruct an image. Our main result is that DPPNs can be evolved/trained
      to compress the weights of a denoising autoencoder from 157684 to roughly
      200 parameters, while achieving a reconstruction accuracy comparable to a
      fully connected network with more than two orders of magnitude more
      parameters. The regularization ability of the DPPN allows it to rediscover
      (approximate) convolutional network architectures embedded within a fully
      connected architecture. Such convolutional architectures are the current
      state of the art for many computer vision applications, so it is
      satisfying that DPPNs are capable of discovering this structure rather
      than having to build it in by design. DPPNs exhibit better generalization
      when tested on the Omniglot dataset after being trained on MNIST, than
      directly encoded fully connected autoencoders. DPPNs are therefore a new
      framework for integrating learning and evolution.
    title: 'Convolution by Evolution: Differentiable Pattern Producing Networks'
    updated: 2016-06-08T14:37:39.000Z
    url: 'http://arxiv.org/abs/1606.02580v1'
    year: 2016
  - authors:
      - name: Alexander Pritzel
      - name: Benigno Uria
      - name: Sriram Srinivasan
      - name: Adrià Puigdomènech
      - name: Oriol Vinyals
      - name: Demis Hassabis
      - name: Daan Wierstra
      - name: Charles Blundell
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1703.01988v1.pdf
    id: 'http://arxiv.org/abs/1703.01988v1'
    links:
      - href: 'http://arxiv.org/abs/1703.01988v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1703.01988v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1703.01988v1'
    published: 2017-03-06T17:23:27.000Z
    summary: >-
      Deep reinforcement learning methods attain super-human performance in a
      wide range of environments. Such methods are grossly inefficient, often
      taking orders of magnitudes more data than humans to achieve reasonable
      performance. We propose Neural Episodic Control: a deep reinforcement
      learning agent that is able to rapidly assimilate new experiences and act
      upon them. Our agent uses a semi-tabular representation of the value
      function: a buffer of past experience containing slowly changing state
      representations and rapidly updated estimates of the value function. We
      show across a wide range of environments that our agent learns
      significantly faster than other state-of-the-art, general purpose deep
      reinforcement learning agents.
    title: Neural Episodic Control
    updated: 2017-03-06T17:23:27.000Z
    url: 'http://arxiv.org/abs/1703.01988v1'
    year: 2017
  - authors:
      - name: Karol Gregor
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
    files:
      - 1611.07507v1.pdf
    id: 'http://arxiv.org/abs/1611.07507v1'
    links:
      - href: 'http://arxiv.org/abs/1611.07507v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1611.07507v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1611.07507v1'
    published: 2016-11-22T20:44:39.000Z
    summary: >-
      In this paper we introduce a new unsupervised reinforcement learning
      method for discovering the set of intrinsic options available to an agent.
      This set is learned by maximizing the number of different states an agent
      can reliably reach, as measured by the mutual information between the set
      of options and option termination states. To this end, we instantiate two
      policy gradient based algorithms, one that creates an explicit embedding
      space of options and one that represents options implicitly. The
      algorithms also provide an explicit measure of empowerment in a given
      state that can be used by an empowerment maximizing agent. The algorithm
      scales well with function approximation and we demonstrate the
      applicability of the algorithm on a range of tasks.
    title: Variational Intrinsic Control
    updated: 2016-11-22T20:44:39.000Z
    url: 'http://arxiv.org/abs/1611.07507v1'
    year: 2016
  - authors:
      - name: Adam Santoro
      - name: Sergey Bartunov
      - name: Matthew Botvinick
      - name: Daan Wierstra
      - name: Timothy Lillicrap
    categories:
      - cs.LG
    files:
      - 1605.06065v1.pdf
    id: 'http://arxiv.org/abs/1605.06065v1'
    links:
      - href: 'http://arxiv.org/abs/1605.06065v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1605.06065v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1605.06065v1'
    published: 2016-05-19T17:44:51.000Z
    summary: >-
      Despite recent breakthroughs in the applications of deep neural networks,
      one setting that presents a persistent challenge is that of "one-shot
      learning." Traditional gradient-based networks require a lot of data to
      learn, often through extensive iterative training. When new data is
      encountered, the models must inefficiently relearn their parameters to
      adequately incorporate the new information without catastrophic
      interference. Architectures with augmented memory capacities, such as
      Neural Turing Machines (NTMs), offer the ability to quickly encode and
      retrieve new information, and hence can potentially obviate the downsides
      of conventional models. Here, we demonstrate the ability of a
      memory-augmented neural network to rapidly assimilate new data, and
      leverage this data to make accurate predictions after only a few samples.
      We also introduce a new method for accessing an external memory that
      focuses on memory content, unlike previous methods that additionally use
      memory location-based focusing mechanisms.
    title: One-shot Learning with Memory-Augmented Neural Networks
    updated: 2016-05-19T17:44:51.000Z
    url: 'http://arxiv.org/abs/1605.06065v1'
    year: 2016
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Alex Graves
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.CV
      - cs.LG
      - cs.NE
    files:
      - 1502.04623v2.pdf
    id: 'http://arxiv.org/abs/1502.04623v2'
    links:
      - href: 'http://arxiv.org/abs/1502.04623v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1502.04623v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1502.04623v2'
    published: 2015-02-16T16:48:56.000Z
    summary: >-
      This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
      network architecture for image generation. DRAW networks combine a novel
      spatial attention mechanism that mimics the foveation of the human eye,
      with a sequential variational auto-encoding framework that allows for the
      iterative construction of complex images. The system substantially
      improves on the state of the art for generative models on MNIST, and, when
      trained on the Street View House Numbers dataset, it generates images that
      cannot be distinguished from real data with the naked eye.
    title: 'DRAW: A Recurrent Neural Network For Image Generation'
    updated: 2015-05-20T15:29:42.000Z
    url: 'http://arxiv.org/abs/1502.04623v2'
    year: 2015
  - authors:
      - name: M. Botvinick
      - name: D. G. T. Barrett
      - name: P. Battaglia
      - name: N. de Freitas
      - name: D. Kumaran
      - name: J. Z Leibo
      - name: T. Lillicrap
      - name: J. Modayil
      - name: S. Mohamed
      - name: N. C. Rabinowitz
      - name: D. J. Rezende
      - name: A. Santoro
      - name: T. Schaul
      - name: C. Summerfield
      - name: G. Wayne
      - name: T. Weber
      - name: D. Wierstra
      - name: S. Legg
      - name: D. Hassabis
    categories:
      - cs.AI
    files:
      - 1711.08378v1.pdf
    id: 'http://arxiv.org/abs/1711.08378v1'
    links:
      - href: 'http://arxiv.org/abs/1711.08378v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1711.08378v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1711.08378v1'
    published: 2017-11-22T16:35:29.000Z
    summary: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
    title: >-
      Building Machines that Learn and Think for Themselves: Commentary on Lake
      et al., Behavioral and Brain Sciences, 2017
    updated: 2017-11-22T16:35:29.000Z
    url: 'http://arxiv.org/abs/1711.08378v1'
    year: 2017
  - authors:
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Tobias Glasmachers
      - name: Yi Sun
      - name: Jürgen Schmidhuber
    categories:
      - stat.ML
      - cs.NE
    files:
      - 1106.4487v1.pdf
    id: 'http://arxiv.org/abs/1106.4487v1'
    links:
      - href: 'http://arxiv.org/abs/1106.4487v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1106.4487v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1106.4487v1'
    published: 2011-06-22T15:55:52.000Z
    summary: >-
      This paper presents Natural Evolution Strategies (NES), a recent family of
      algorithms that constitute a more principled approach to black-box
      optimization than established evolutionary algorithms. NES maintains a
      parameterized distribution on the set of solution candidates, and the
      natural gradient is used to update the distribution's parameters in the
      direction of higher expected fitness. We introduce a collection of
      techniques that address issues of convergence, robustness, sample
      complexity, computational complexity and sensitivity to hyperparameters.
      This paper explores a number of implementations of the NES family, ranging
      from general-purpose multi-variate normal distributions to heavy-tailed
      and separable distributions tailored towards global optimization and
      search in high dimensional spaces, respectively. Experimental results show
      best published performance on various standard benchmarks, as well as
      competitive performance on others.
    title: Natural Evolution Strategies
    updated: 2011-06-22T15:55:52.000Z
    url: 'http://arxiv.org/abs/1106.4487v1'
    year: 2011
  - authors:
      - name: Ivo Danihelka
      - name: Balaji Lakshminarayanan
      - name: Benigno Uria
      - name: Daan Wierstra
      - name: Peter Dayan
    categories:
      - cs.LG
    files:
      - 1705.05263v1.pdf
    id: 'http://arxiv.org/abs/1705.05263v1'
    links:
      - href: 'http://arxiv.org/abs/1705.05263v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1705.05263v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1705.05263v1'
    published: 2017-05-15T14:24:01.000Z
    summary: >-
      We train a generator by maximum likelihood and we also train the same
      generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein
      distances. We show that an independent critic trained to approximate
      Wasserstein distance between the validation set and the generator
      distribution helps detect overfitting. Finally, we use ideas from the
      one-shot learning literature to develop a novel fast learning critic.
    title: Comparison of Maximum Likelihood and GAN-based training of Real NVPs
    updated: 2017-05-15T14:24:01.000Z
    url: 'http://arxiv.org/abs/1705.05263v1'
    year: 2017
  - authors:
      - name: Oriol Vinyals
      - name: Charles Blundell
      - name: Timothy Lillicrap
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1606.04080v2.pdf
    id: 'http://arxiv.org/abs/1606.04080v2'
    links:
      - href: 'http://arxiv.org/abs/1606.04080v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04080v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04080v2'
    published: 2016-06-13T19:34:22.000Z
    summary: >-
      Learning from a few examples remains a key challenge in machine learning.
      Despite recent advances in important domains such as vision and language,
      the standard supervised deep learning paradigm does not offer a
      satisfactory solution for learning new concepts rapidly from little data.
      In this work, we employ ideas from metric learning based on deep neural
      features and from recent advances that augment neural networks with
      external memories. Our framework learns a network that maps a small
      labelled support set and an unlabelled example to its label, obviating the
      need for fine-tuning to adapt to new class types. We then define one-shot
      learning problems on vision (using Omniglot, ImageNet) and language tasks.
      Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2%
      and from 88.0% to 93.8% on Omniglot compared to competing approaches. We
      also demonstrate the usefulness of the same model on language modeling by
      introducing a one-shot task on the Penn Treebank.
    title: Matching Networks for One Shot Learning
    updated: 2017-12-29T17:45:19.000Z
    url: 'http://arxiv.org/abs/1606.04080v2'
    year: 2016
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Ivo Danihelka
      - name: Karol Gregor
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
    files:
      - 1603.05106v2.pdf
    id: 'http://arxiv.org/abs/1603.05106v2'
    links:
      - href: 'http://arxiv.org/abs/1603.05106v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1603.05106v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1603.05106v2'
    published: 2016-03-16T14:10:00.000Z
    summary: >-
      Humans have an impressive ability to reason about new concepts and
      experiences from just a single example. In particular, humans have an
      ability for one-shot generalization: an ability to encounter a new
      concept, understand its structure, and then be able to generate compelling
      alternative variations of the concept. We develop machine learning systems
      with this important capacity by developing new deep generative models,
      models that combine the representational power of deep learning with the
      inferential power of Bayesian reasoning. We develop a class of sequential
      generative models that are built on the principles of feedback and
      attention. These two characteristics lead to generative models that are
      among the state-of-the art in density estimation and image generation. We
      demonstrate the one-shot generalization ability of our models using three
      tasks: unconditional sampling, generating new exemplars of a given
      concept, and generating new exemplars of a family of concepts. In all
      cases our models are able to generate compelling and diverse
      samples---having seen new examples just once---providing an important
      class of general-purpose models for one-shot machine learning.
    title: One-Shot Generalization in Deep Generative Models
    updated: 2016-05-25T12:57:19.000Z
    url: 'http://arxiv.org/abs/1603.05106v2'
    year: 2016
  - authors:
      - name: Karol Gregor
      - name: Frederic Besse
      - name: Danilo Jimenez Rezende
      - name: Ivo Danihelka
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.CV
      - cs.LG
    files:
      - 1604.08772v1.pdf
    id: 'http://arxiv.org/abs/1604.08772v1'
    links:
      - href: 'http://arxiv.org/abs/1604.08772v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1604.08772v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1604.08772v1'
    published: 2016-04-29T11:02:52.000Z
    summary: >-
      We introduce a simple recurrent variational auto-encoder architecture that
      significantly improves image modeling. The system represents the
      state-of-the-art in latent variable models for both the ImageNet and
      Omniglot datasets. We show that it naturally separates global conceptual
      information from lower level details, thus addressing one of the
      fundamentally desired properties of unsupervised learning. Furthermore,
      the possibility of restricting ourselves to storing only global
      information about an image allows us to achieve high quality 'conceptual
      compression'.
    title: Towards Conceptual Compression
    updated: 2016-04-29T11:02:52.000Z
    url: 'http://arxiv.org/abs/1604.08772v1'
    year: 2016
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
      - stat.CO
      - stat.ME
    files:
      - 1401.4082v3.pdf
    id: 'http://arxiv.org/abs/1401.4082v3'
    links:
      - href: 'http://arxiv.org/abs/1401.4082v3'
        title: ''
      - href: 'http://arxiv.org/pdf/1401.4082v3'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1401.4082v3'
    published: 2014-01-16T16:33:23.000Z
    summary: >-
      We marry ideas from deep neural networks and approximate Bayesian
      inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning.
      Our algorithm introduces a recognition model to represent approximate
      posterior distributions, and that acts as a stochastic encoder of the
      data. We develop stochastic back-propagation -- rules for back-propagation
      through stochastic variables -- and use this to develop an algorithm that
      allows for joint optimisation of the parameters of both the generative and
      recognition model. We demonstrate on several real-world data sets that the
      model generates realistic samples, provides accurate imputations of
      missing data and is a useful tool for high-dimensional data visualisation.
    title: >-
      Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models
    updated: 2014-05-30T10:00:36.000Z
    url: 'http://arxiv.org/abs/1401.4082v3'
    year: 2014
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Charles Blundell
      - name: Yori Zwols
      - name: David Ha
      - name: Andrei A. Rusu
      - name: Alexander Pritzel
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.LG
    files:
      - 1701.08734v1.pdf
    id: 'http://arxiv.org/abs/1701.08734v1'
    links:
      - href: 'http://arxiv.org/abs/1701.08734v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1701.08734v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1701.08734v1'
    published: 2017-01-30T18:06:07.000Z
    summary: >-
      For artificial general intelligence (AGI) it would be efficient if
      multiple users trained the same giant neural network, permitting parameter
      reuse, without catastrophic forgetting. PathNet is a first step in this
      direction. It is a neural network algorithm that uses agents embedded in
      the neural network whose task is to discover which parts of the network to
      re-use for new tasks. Agents are pathways (views) through the network
      which determine the subset of parameters that are used and updated by the
      forwards and backwards passes of the backpropogation algorithm. During
      learning, a tournament selection genetic algorithm is used to select
      pathways through the neural network for replication and mutation. Pathway
      fitness is the performance of that pathway measured according to a cost
      function. We demonstrate successful transfer learning; fixing the
      parameters along a path learned on task A and re-evolving a new population
      of paths for task B, allows task B to be learned faster than it could be
      learned from scratch or after fine-tuning. Paths evolved on task B re-use
      parts of the optimal path evolved on task A. Positive transfer was
      demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
      classification tasks, and a set of Atari and Labyrinth reinforcement
      learning tasks, suggesting PathNets have general applicability for neural
      network training. Finally, PathNet also significantly improves the
      robustness to hyperparameter choices of a parallel asynchronous
      reinforcement learning algorithm (A3C).
    title: 'PathNet: Evolution Channels Gradient Descent in Super Neural Networks'
    updated: 2017-01-30T18:06:07.000Z
    url: 'http://arxiv.org/abs/1701.08734v1'
    year: 2017
  - authors:
      - name: Silvia Chiappa
      - name: Sébastien Racaniere
      - name: Daan Wierstra
      - name: Shakir Mohamed
    categories:
      - cs.AI
      - cs.LG
      - stat.ML
    files:
      - 1704.02254v2.pdf
    id: 'http://arxiv.org/abs/1704.02254v2'
    links:
      - href: 'http://arxiv.org/abs/1704.02254v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1704.02254v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1704.02254v2'
    published: 2017-04-07T14:53:54.000Z
    summary: >-
      Models that can simulate how environments change in response to actions
      can be used by agents to plan and act efficiently. We improve on previous
      environment simulators from high-dimensional pixel observations by
      introducing recurrent neural networks that are able to make temporally and
      spatially coherent predictions for hundreds of time-steps into the future.
      We present an in-depth analysis of the factors affecting performance,
      providing the most extensive attempt to advance the understanding of the
      properties of these models. We address the issue of computationally
      inefficiency with a model that does not need to generate a
      high-dimensional image at each time-step. We show that our approach can be
      used to improve exploration and is adaptable to many diverse environments,
      namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.
    title: Recurrent Environment Simulators
    updated: 2017-04-19T15:43:32.000Z
    url: 'http://arxiv.org/abs/1704.02254v2'
    year: 2017
  - authors:
      - name: M. Botvinick
      - name: D. G. T. Barrett
      - name: P. Battaglia
      - name: N. de Freitas
      - name: D. Kumaran
      - name: J. Z Leibo
      - name: T. Lillicrap
      - name: J. Modayil
      - name: S. Mohamed
      - name: N. C. Rabinowitz
      - name: D. J. Rezende
      - name: A. Santoro
      - name: T. Schaul
      - name: C. Summerfield
      - name: G. Wayne
      - name: T. Weber
      - name: D. Wierstra
      - name: S. Legg
      - name: D. Hassabis
    categories:
      - cs.AI
    files:
      - 1711.08378v1.pdf
    id: 'http://arxiv.org/abs/1711.08378v1'
    links:
      - href: 'http://arxiv.org/abs/1711.08378v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1711.08378v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1711.08378v1'
    published: 2017-11-22T16:35:29.000Z
    summary: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
    title: >-
      Building Machines that Learn and Think for Themselves: Commentary on Lake
      et al., Behavioral and Brain Sciences, 2017
    updated: 2017-11-22T16:35:29.000Z
    url: 'http://arxiv.org/abs/1711.08378v1'
    year: 2017
  - authors:
      - name: Théophane Weber
      - name: Sébastien Racanière
      - name: David P. Reichert
      - name: Lars Buesing
      - name: Arthur Guez
      - name: Danilo Jimenez Rezende
      - name: Adria Puigdomènech Badia
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Yujia Li
      - name: Razvan Pascanu
      - name: Peter Battaglia
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
      - stat.ML
    files:
      - 1707.06203v1.pdf
    id: 'http://arxiv.org/abs/1707.06203v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06203v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06203v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06203v1'
    published: 2017-07-19T17:12:56.000Z
    summary: >-
      We introduce Imagination-Augmented Agents (I2As), a novel architecture for
      deep reinforcement learning combining model-free and model-based aspects.
      In contrast to most existing model-based reinforcement learning and
      planning methods, which prescribe how a model should be used to arrive at
      a policy, I2As learn to interpret predictions from a learned environment
      model to construct implicit plans in arbitrary ways, by using the
      predictions as additional context in deep policy networks. I2As show
      improved data efficiency, performance, and robustness to model
      misspecification compared to several baselines.
    title: Imagination-Augmented Agents for Deep Reinforcement Learning
    updated: 2017-07-19T17:12:56.000Z
    url: 'http://arxiv.org/abs/1707.06203v1'
    year: 2017
  - authors:
      - name: Razvan Pascanu
      - name: Yujia Li
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Lars Buesing
      - name: Sebastien Racanière
      - name: David Reichert
      - name: Théophane Weber
      - name: Daan Wierstra
      - name: Peter Battaglia
    categories:
      - cs.AI
      - cs.LG
      - cs.NE
      - stat.ML
    files:
      - 1707.06170v1.pdf
    id: 'http://arxiv.org/abs/1707.06170v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06170v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06170v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06170v1'
    published: 2017-07-19T15:52:35.000Z
    summary: >-
      Conventional wisdom holds that model-based planning is a powerful approach
      to sequential decision-making. It is often very challenging in practice,
      however, because while a model can be used to evaluate a plan, it does not
      prescribe how to construct a plan. Here we introduce the
      "Imagination-based Planner", the first model-based, sequential
      decision-making agent that can learn to construct, evaluate, and execute
      plans. Before any action, it can perform a variable number of imagination
      steps, which involve proposing an imagined action and evaluating it with
      its model-based imagination. All imagined actions and outcomes are
      aggregated, iteratively, into a "plan context" which conditions future
      real and imagined actions. The agent can even decide how to imagine:
      testing out alternative imagined actions, chaining sequences of actions
      together, or building a more complex "imagination tree" by navigating
      flexibly among the previously imagined states using a learned policy. And
      our agent can learn to plan economically, jointly optimizing for external
      rewards and computational costs associated with using its imagination. We
      show that our architecture can learn to solve a challenging continuous
      control problem, and also learn elaborate planning strategies in a
      discrete maze-solving task. Our work opens a new direction toward learning
      the components of a model-based planning system and how to use them.
    title: Learning model-based planning from scratch
    updated: 2017-07-19T15:52:35.000Z
    url: 'http://arxiv.org/abs/1707.06170v1'
    year: 2017
  - authors:
      - name: Ivo Danihelka
      - name: Balaji Lakshminarayanan
      - name: Benigno Uria
      - name: Daan Wierstra
      - name: Peter Dayan
    categories:
      - cs.LG
    files:
      - 1705.05263v1.pdf
    id: 'http://arxiv.org/abs/1705.05263v1'
    links:
      - href: 'http://arxiv.org/abs/1705.05263v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1705.05263v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1705.05263v1'
    published: 2017-05-15T14:24:01.000Z
    summary: >-
      We train a generator by maximum likelihood and we also train the same
      generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein
      distances. We show that an independent critic trained to approximate
      Wasserstein distance between the validation set and the generator
      distribution helps detect overfitting. Finally, we use ideas from the
      one-shot learning literature to develop a novel fast learning critic.
    title: Comparison of Maximum Likelihood and GAN-based training of Real NVPs
    updated: 2017-05-15T14:24:01.000Z
    url: 'http://arxiv.org/abs/1705.05263v1'
    year: 2017
  - authors:
      - name: Silvia Chiappa
      - name: Sébastien Racaniere
      - name: Daan Wierstra
      - name: Shakir Mohamed
    categories:
      - cs.AI
      - cs.LG
      - stat.ML
    files:
      - 1704.02254v2.pdf
    id: 'http://arxiv.org/abs/1704.02254v2'
    links:
      - href: 'http://arxiv.org/abs/1704.02254v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1704.02254v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1704.02254v2'
    published: 2017-04-07T14:53:54.000Z
    summary: >-
      Models that can simulate how environments change in response to actions
      can be used by agents to plan and act efficiently. We improve on previous
      environment simulators from high-dimensional pixel observations by
      introducing recurrent neural networks that are able to make temporally and
      spatially coherent predictions for hundreds of time-steps into the future.
      We present an in-depth analysis of the factors affecting performance,
      providing the most extensive attempt to advance the understanding of the
      properties of these models. We address the issue of computationally
      inefficiency with a model that does not need to generate a
      high-dimensional image at each time-step. We show that our approach can be
      used to improve exploration and is adaptable to many diverse environments,
      namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.
    title: Recurrent Environment Simulators
    updated: 2017-04-19T15:43:32.000Z
    url: 'http://arxiv.org/abs/1704.02254v2'
    year: 2017
  - authors:
      - name: Alexander Pritzel
      - name: Benigno Uria
      - name: Sriram Srinivasan
      - name: Adrià Puigdomènech
      - name: Oriol Vinyals
      - name: Demis Hassabis
      - name: Daan Wierstra
      - name: Charles Blundell
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1703.01988v1.pdf
    id: 'http://arxiv.org/abs/1703.01988v1'
    links:
      - href: 'http://arxiv.org/abs/1703.01988v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1703.01988v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1703.01988v1'
    published: 2017-03-06T17:23:27.000Z
    summary: >-
      Deep reinforcement learning methods attain super-human performance in a
      wide range of environments. Such methods are grossly inefficient, often
      taking orders of magnitudes more data than humans to achieve reasonable
      performance. We propose Neural Episodic Control: a deep reinforcement
      learning agent that is able to rapidly assimilate new experiences and act
      upon them. Our agent uses a semi-tabular representation of the value
      function: a buffer of past experience containing slowly changing state
      representations and rapidly updated estimates of the value function. We
      show across a wide range of environments that our agent learns
      significantly faster than other state-of-the-art, general purpose deep
      reinforcement learning agents.
    title: Neural Episodic Control
    updated: 2017-03-06T17:23:27.000Z
    url: 'http://arxiv.org/abs/1703.01988v1'
    year: 2017
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Charles Blundell
      - name: Yori Zwols
      - name: David Ha
      - name: Andrei A. Rusu
      - name: Alexander Pritzel
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.LG
    files:
      - 1701.08734v1.pdf
    id: 'http://arxiv.org/abs/1701.08734v1'
    links:
      - href: 'http://arxiv.org/abs/1701.08734v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1701.08734v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1701.08734v1'
    published: 2017-01-30T18:06:07.000Z
    summary: >-
      For artificial general intelligence (AGI) it would be efficient if
      multiple users trained the same giant neural network, permitting parameter
      reuse, without catastrophic forgetting. PathNet is a first step in this
      direction. It is a neural network algorithm that uses agents embedded in
      the neural network whose task is to discover which parts of the network to
      re-use for new tasks. Agents are pathways (views) through the network
      which determine the subset of parameters that are used and updated by the
      forwards and backwards passes of the backpropogation algorithm. During
      learning, a tournament selection genetic algorithm is used to select
      pathways through the neural network for replication and mutation. Pathway
      fitness is the performance of that pathway measured according to a cost
      function. We demonstrate successful transfer learning; fixing the
      parameters along a path learned on task A and re-evolving a new population
      of paths for task B, allows task B to be learned faster than it could be
      learned from scratch or after fine-tuning. Paths evolved on task B re-use
      parts of the optimal path evolved on task A. Positive transfer was
      demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
      classification tasks, and a set of Atari and Labyrinth reinforcement
      learning tasks, suggesting PathNets have general applicability for neural
      network training. Finally, PathNet also significantly improves the
      robustness to hyperparameter choices of a parallel asynchronous
      reinforcement learning algorithm (A3C).
    title: 'PathNet: Evolution Channels Gradient Descent in Super Neural Networks'
    updated: 2017-01-30T18:06:07.000Z
    url: 'http://arxiv.org/abs/1701.08734v1'
    year: 2017
  - authors:
      - name: Karol Gregor
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
    files:
      - 1611.07507v1.pdf
    id: 'http://arxiv.org/abs/1611.07507v1'
    links:
      - href: 'http://arxiv.org/abs/1611.07507v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1611.07507v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1611.07507v1'
    published: 2016-11-22T20:44:39.000Z
    summary: >-
      In this paper we introduce a new unsupervised reinforcement learning
      method for discovering the set of intrinsic options available to an agent.
      This set is learned by maximizing the number of different states an agent
      can reliably reach, as measured by the mutual information between the set
      of options and option termination states. To this end, we instantiate two
      policy gradient based algorithms, one that creates an explicit embedding
      space of options and one that represents options implicitly. The
      algorithms also provide an explicit measure of empowerment in a given
      state that can be used by an empowerment maximizing agent. The algorithm
      scales well with function approximation and we demonstrate the
      applicability of the algorithm on a range of tasks.
    title: Variational Intrinsic Control
    updated: 2016-11-22T20:44:39.000Z
    url: 'http://arxiv.org/abs/1611.07507v1'
    year: 2016
  - authors:
      - name: Charles Blundell
      - name: Benigno Uria
      - name: Alexander Pritzel
      - name: Yazhe Li
      - name: Avraham Ruderman
      - name: Joel Z Leibo
      - name: Jack Rae
      - name: Daan Wierstra
      - name: Demis Hassabis
    categories:
      - stat.ML
      - cs.LG
      - q-bio.NC
    files:
      - 1606.04460v1.pdf
    id: 'http://arxiv.org/abs/1606.04460v1'
    links:
      - href: 'http://arxiv.org/abs/1606.04460v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04460v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04460v1'
    published: 2016-06-14T17:03:46.000Z
    summary: >-
      State of the art deep reinforcement learning algorithms take many millions
      of interactions to attain human-level performance. Humans, on the other
      hand, can very quickly exploit highly rewarding nuances of an environment
      upon first discovery. In the brain, such rapid learning is thought to
      depend on the hippocampus and its capacity for episodic memory. Here we
      investigate whether a simple model of hippocampal episodic control can
      learn to solve difficult sequential decision-making tasks. We demonstrate
      that it not only attains a highly rewarding strategy significantly faster
      than state-of-the-art deep reinforcement learning algorithms, but also
      achieves a higher overall reward on some of the more challenging domains.
    title: Model-Free Episodic Control
    updated: 2016-06-14T17:03:46.000Z
    url: 'http://arxiv.org/abs/1606.04460v1'
    year: 2016
  - authors:
      - name: Oriol Vinyals
      - name: Charles Blundell
      - name: Timothy Lillicrap
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1606.04080v2.pdf
    id: 'http://arxiv.org/abs/1606.04080v2'
    links:
      - href: 'http://arxiv.org/abs/1606.04080v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04080v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04080v2'
    published: 2016-06-13T19:34:22.000Z
    summary: >-
      Learning from a few examples remains a key challenge in machine learning.
      Despite recent advances in important domains such as vision and language,
      the standard supervised deep learning paradigm does not offer a
      satisfactory solution for learning new concepts rapidly from little data.
      In this work, we employ ideas from metric learning based on deep neural
      features and from recent advances that augment neural networks with
      external memories. Our framework learns a network that maps a small
      labelled support set and an unlabelled example to its label, obviating the
      need for fine-tuning to adapt to new class types. We then define one-shot
      learning problems on vision (using Omniglot, ImageNet) and language tasks.
      Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2%
      and from 88.0% to 93.8% on Omniglot compared to competing approaches. We
      also demonstrate the usefulness of the same model on language modeling by
      introducing a one-shot task on the Penn Treebank.
    title: Matching Networks for One Shot Learning
    updated: 2017-12-29T17:45:19.000Z
    url: 'http://arxiv.org/abs/1606.04080v2'
    year: 2016
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Malcolm Reynolds
      - name: Frederic Besse
      - name: David Pfau
      - name: Max Jaderberg
      - name: Marc Lanctot
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.CV
      - cs.LG
    files:
      - 1606.02580v1.pdf
    id: 'http://arxiv.org/abs/1606.02580v1'
    links:
      - href: 'http://arxiv.org/abs/1606.02580v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.02580v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.02580v1'
    published: 2016-06-08T14:37:39.000Z
    summary: >-
      In this work we introduce a differentiable version of the Compositional
      Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the
      topology of a DPPN is evolved but the weights are learned. A Lamarckian
      algorithm, that combines evolution and learning, produces DPPNs to
      reconstruct an image. Our main result is that DPPNs can be evolved/trained
      to compress the weights of a denoising autoencoder from 157684 to roughly
      200 parameters, while achieving a reconstruction accuracy comparable to a
      fully connected network with more than two orders of magnitude more
      parameters. The regularization ability of the DPPN allows it to rediscover
      (approximate) convolutional network architectures embedded within a fully
      connected architecture. Such convolutional architectures are the current
      state of the art for many computer vision applications, so it is
      satisfying that DPPNs are capable of discovering this structure rather
      than having to build it in by design. DPPNs exhibit better generalization
      when tested on the Omniglot dataset after being trained on MNIST, than
      directly encoded fully connected autoencoders. DPPNs are therefore a new
      framework for integrating learning and evolution.
    title: 'Convolution by Evolution: Differentiable Pattern Producing Networks'
    updated: 2016-06-08T14:37:39.000Z
    url: 'http://arxiv.org/abs/1606.02580v1'
    year: 2016
  - authors:
      - name: Adam Santoro
      - name: Sergey Bartunov
      - name: Matthew Botvinick
      - name: Daan Wierstra
      - name: Timothy Lillicrap
    categories:
      - cs.LG
    files:
      - 1605.06065v1.pdf
    id: 'http://arxiv.org/abs/1605.06065v1'
    links:
      - href: 'http://arxiv.org/abs/1605.06065v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1605.06065v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1605.06065v1'
    published: 2016-05-19T17:44:51.000Z
    summary: >-
      Despite recent breakthroughs in the applications of deep neural networks,
      one setting that presents a persistent challenge is that of "one-shot
      learning." Traditional gradient-based networks require a lot of data to
      learn, often through extensive iterative training. When new data is
      encountered, the models must inefficiently relearn their parameters to
      adequately incorporate the new information without catastrophic
      interference. Architectures with augmented memory capacities, such as
      Neural Turing Machines (NTMs), offer the ability to quickly encode and
      retrieve new information, and hence can potentially obviate the downsides
      of conventional models. Here, we demonstrate the ability of a
      memory-augmented neural network to rapidly assimilate new data, and
      leverage this data to make accurate predictions after only a few samples.
      We also introduce a new method for accessing an external memory that
      focuses on memory content, unlike previous methods that additionally use
      memory location-based focusing mechanisms.
    title: One-shot Learning with Memory-Augmented Neural Networks
    updated: 2016-05-19T17:44:51.000Z
    url: 'http://arxiv.org/abs/1605.06065v1'
    year: 2016
  - authors:
      - name: Karol Gregor
      - name: Frederic Besse
      - name: Danilo Jimenez Rezende
      - name: Ivo Danihelka
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.CV
      - cs.LG
    files:
      - 1604.08772v1.pdf
    id: 'http://arxiv.org/abs/1604.08772v1'
    links:
      - href: 'http://arxiv.org/abs/1604.08772v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1604.08772v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1604.08772v1'
    published: 2016-04-29T11:02:52.000Z
    summary: >-
      We introduce a simple recurrent variational auto-encoder architecture that
      significantly improves image modeling. The system represents the
      state-of-the-art in latent variable models for both the ImageNet and
      Omniglot datasets. We show that it naturally separates global conceptual
      information from lower level details, thus addressing one of the
      fundamentally desired properties of unsupervised learning. Furthermore,
      the possibility of restricting ourselves to storing only global
      information about an image allows us to achieve high quality 'conceptual
      compression'.
    title: Towards Conceptual Compression
    updated: 2016-04-29T11:02:52.000Z
    url: 'http://arxiv.org/abs/1604.08772v1'
    year: 2016
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Ivo Danihelka
      - name: Karol Gregor
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
    files:
      - 1603.05106v2.pdf
    id: 'http://arxiv.org/abs/1603.05106v2'
    links:
      - href: 'http://arxiv.org/abs/1603.05106v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1603.05106v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1603.05106v2'
    published: 2016-03-16T14:10:00.000Z
    summary: >-
      Humans have an impressive ability to reason about new concepts and
      experiences from just a single example. In particular, humans have an
      ability for one-shot generalization: an ability to encounter a new
      concept, understand its structure, and then be able to generate compelling
      alternative variations of the concept. We develop machine learning systems
      with this important capacity by developing new deep generative models,
      models that combine the representational power of deep learning with the
      inferential power of Bayesian reasoning. We develop a class of sequential
      generative models that are built on the principles of feedback and
      attention. These two characteristics lead to generative models that are
      among the state-of-the art in density estimation and image generation. We
      demonstrate the one-shot generalization ability of our models using three
      tasks: unconditional sampling, generating new exemplars of a given
      concept, and generating new exemplars of a family of concepts. In all
      cases our models are able to generate compelling and diverse
      samples---having seen new examples just once---providing an important
      class of general-purpose models for one-shot machine learning.
    title: One-Shot Generalization in Deep Generative Models
    updated: 2016-05-25T12:57:19.000Z
    url: 'http://arxiv.org/abs/1603.05106v2'
    year: 2016
  - authors:
      - name: Timothy P. Lillicrap
      - name: Jonathan J. Hunt
      - name: Alexander Pritzel
      - name: Nicolas Heess
      - name: Tom Erez
      - name: Yuval Tassa
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1509.02971v5.pdf
    id: 'http://arxiv.org/abs/1509.02971v5'
    links:
      - href: 'http://arxiv.org/abs/1509.02971v5'
        title: ''
      - href: 'http://arxiv.org/pdf/1509.02971v5'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1509.02971v5'
    published: 2015-09-09T23:01:36.000Z
    summary: >-
      We adapt the ideas underlying the success of Deep Q-Learning to the
      continuous action domain. We present an actor-critic, model-free algorithm
      based on the deterministic policy gradient that can operate over
      continuous action spaces. Using the same learning algorithm, network
      architecture and hyper-parameters, our algorithm robustly solves more than
      20 simulated physics tasks, including classic problems such as cartpole
      swing-up, dexterous manipulation, legged locomotion and car driving. Our
      algorithm is able to find policies whose performance is competitive with
      those found by a planning algorithm with full access to the dynamics of
      the domain and its derivatives. We further demonstrate that for many of
      the tasks the algorithm can learn policies end-to-end: directly from raw
      pixel inputs.
    title: Continuous control with deep reinforcement learning
    updated: 2016-02-29T18:45:53.000Z
    url: 'http://arxiv.org/abs/1509.02971v5'
    year: 2015
  - authors:
      - name: Charles Blundell
      - name: Julien Cornebise
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.LG
    files:
      - 1505.05424v2.pdf
    id: 'http://arxiv.org/abs/1505.05424v2'
    links:
      - href: 'http://arxiv.org/abs/1505.05424v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1505.05424v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1505.05424v2'
    published: 2015-05-20T15:39:48.000Z
    summary: >-
      We introduce a new, efficient, principled and backpropagation-compatible
      algorithm for learning a probability distribution on the weights of a
      neural network, called Bayes by Backprop. It regularises the weights by
      minimising a compression cost, known as the variational free energy or the
      expected lower bound on the marginal likelihood. We show that this
      principled kind of regularisation yields comparable performance to dropout
      on MNIST classification. We then demonstrate how the learnt uncertainty in
      the weights can be used to improve generalisation in non-linear regression
      problems, and how this weight uncertainty can be used to drive the
      exploration-exploitation trade-off in reinforcement learning.
    title: Weight Uncertainty in Neural Networks
    updated: 2015-05-21T14:07:23.000Z
    url: 'http://arxiv.org/abs/1505.05424v2'
    year: 2015
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Alex Graves
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.CV
      - cs.LG
      - cs.NE
    files:
      - 1502.04623v2.pdf
    id: 'http://arxiv.org/abs/1502.04623v2'
    links:
      - href: 'http://arxiv.org/abs/1502.04623v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1502.04623v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1502.04623v2'
    published: 2015-02-16T16:48:56.000Z
    summary: >-
      This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
      network architecture for image generation. DRAW networks combine a novel
      spatial attention mechanism that mimics the foveation of the human eye,
      with a sequential variational auto-encoding framework that allows for the
      iterative construction of complex images. The system substantially
      improves on the state of the art for generative models on MNIST, and, when
      trained on the Street View House Numbers dataset, it generates images that
      cannot be distinguished from real data with the naked eye.
    title: 'DRAW: A Recurrent Neural Network For Image Generation'
    updated: 2015-05-20T15:29:42.000Z
    url: 'http://arxiv.org/abs/1502.04623v2'
    year: 2015
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
      - stat.CO
      - stat.ME
    files:
      - 1401.4082v3.pdf
    id: 'http://arxiv.org/abs/1401.4082v3'
    links:
      - href: 'http://arxiv.org/abs/1401.4082v3'
        title: ''
      - href: 'http://arxiv.org/pdf/1401.4082v3'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1401.4082v3'
    published: 2014-01-16T16:33:23.000Z
    summary: >-
      We marry ideas from deep neural networks and approximate Bayesian
      inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning.
      Our algorithm introduces a recognition model to represent approximate
      posterior distributions, and that acts as a stochastic encoder of the
      data. We develop stochastic back-propagation -- rules for back-propagation
      through stochastic variables -- and use this to develop an algorithm that
      allows for joint optimisation of the parameters of both the generative and
      recognition model. We demonstrate on several real-world data sets that the
      model generates realistic samples, provides accurate imputations of
      missing data and is a useful tool for high-dimensional data visualisation.
    title: >-
      Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models
    updated: 2014-05-30T10:00:36.000Z
    url: 'http://arxiv.org/abs/1401.4082v3'
    year: 2014
  - authors:
      - name: Volodymyr Mnih
      - name: Koray Kavukcuoglu
      - name: David Silver
      - name: Alex Graves
      - name: Ioannis Antonoglou
      - name: Daan Wierstra
      - name: Martin Riedmiller
    categories:
      - cs.LG
    files:
      - 1312.5602v1.pdf
    id: 'http://arxiv.org/abs/1312.5602v1'
    links:
      - href: 'http://arxiv.org/abs/1312.5602v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1312.5602v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1312.5602v1'
    published: 2013-12-19T16:00:08.000Z
    summary: >-
      We present the first deep learning model to successfully learn control
      policies directly from high-dimensional sensory input using reinforcement
      learning. The model is a convolutional neural network, trained with a
      variant of Q-learning, whose input is raw pixels and whose output is a
      value function estimating future rewards. We apply our method to seven
      Atari 2600 games from the Arcade Learning Environment, with no adjustment
      of the architecture or learning algorithm. We find that it outperforms all
      previous approaches on six of the games and surpasses a human expert on
      three of them.
    title: Playing Atari with Deep Reinforcement Learning
    updated: 2013-12-19T16:00:08.000Z
    url: 'http://arxiv.org/abs/1312.5602v1'
    year: 2013
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Andriy Mnih
      - name: Charles Blundell
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1310.8499v2.pdf
    id: 'http://arxiv.org/abs/1310.8499v2'
    links:
      - href: 'http://arxiv.org/abs/1310.8499v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1310.8499v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1310.8499v2'
    published: 2013-10-31T13:47:30.000Z
    summary: >-
      We introduce a deep, generative autoencoder capable of learning
      hierarchies of distributed representations from data. Successive deep
      stochastic hidden layers are equipped with autoregressive connections,
      which enable the model to be sampled from quickly and exactly via
      ancestral sampling. We derive an efficient approximate parameter
      estimation method based on the minimum description length (MDL) principle,
      which can be seen as maximising a variational lower bound on the
      log-likelihood, with a feedforward neural network implementing approximate
      inference. We demonstrate state-of-the-art generative performance on a
      number of classic data sets: several UCI data sets, MNIST and Atari 2600
      games.
    title: Deep AutoRegressive Networks
    updated: 2014-05-20T16:22:43.000Z
    url: 'http://arxiv.org/abs/1310.8499v2'
    year: 2013
  - authors:
      - name: Yi Sun
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Juergen Schmidhuber
    categories:
      - cs.AI
    files:
      - 1209.5853v1.pdf
    id: 'http://arxiv.org/abs/1209.5853v1'
    links:
      - href: 'http://arxiv.org/abs/1209.5853v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1209.5853v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1209.5853v1'
    published: 2012-09-26T07:42:06.000Z
    summary: >-
      Efficient Natural Evolution Strategies (eNES) is a novel alternative to
      conventional evolutionary algorithms, using the natural gradient to adapt
      the mutation distribution. Unlike previous methods based on natural
      gradients, eNES uses a fast algorithm to calculate the inverse of the
      exact Fisher information matrix, thus increasing both robustness and
      performance of its evolution gradient estimation, even in higher
      dimensions. Additional novel aspects of eNES include optimal fitness
      baselines and importance mixing (a procedure for updating the population
      with very few fitness evaluations). The algorithm yields competitive
      results on both unimodal and multimodal benchmarks.
    title: Efficient Natural Evolution Strategies
    updated: 2012-09-26T07:42:06.000Z
    url: 'http://arxiv.org/abs/1209.5853v1'
    year: 2012
  - authors:
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Tobias Glasmachers
      - name: Yi Sun
      - name: Jürgen Schmidhuber
    categories:
      - stat.ML
      - cs.NE
    files:
      - 1106.4487v1.pdf
    id: 'http://arxiv.org/abs/1106.4487v1'
    links:
      - href: 'http://arxiv.org/abs/1106.4487v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1106.4487v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1106.4487v1'
    published: 2011-06-22T15:55:52.000Z
    summary: >-
      This paper presents Natural Evolution Strategies (NES), a recent family of
      algorithms that constitute a more principled approach to black-box
      optimization than established evolutionary algorithms. NES maintains a
      parameterized distribution on the set of solution candidates, and the
      natural gradient is used to update the distribution's parameters in the
      direction of higher expected fitness. We introduce a collection of
      techniques that address issues of convergence, robustness, sample
      complexity, computational complexity and sensitivity to hyperparameters.
      This paper explores a number of implementations of the NES family, ranging
      from general-purpose multi-variate normal distributions to heavy-tailed
      and separable distributions tailored towards global optimization and
      search in high dimensional spaces, respectively. Experimental results show
      best published performance on various standard benchmarks, as well as
      competitive performance on others.
    title: Natural Evolution Strategies
    updated: 2011-06-22T15:55:52.000Z
    url: 'http://arxiv.org/abs/1106.4487v1'
    year: 2011
  - authors:
      - name: Juergen Schmidhuber
      - name: Matteo Gagliolo
      - name: Daan Wierstra
      - name: Faustino Gomez
    categories:
      - cs.NE
      - F.1.1; I.2.6
    files:
      - 0512062v1.pdf
    id: 'http://arxiv.org/abs/cs/0512062v1'
    links:
      - href: 'http://arxiv.org/abs/cs/0512062v1'
        title: ''
      - href: 'http://arxiv.org/pdf/cs/0512062v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/cs/0512062v1'
    published: 2005-12-15T15:05:22.000Z
    summary: >-
      Traditional Support Vector Machines (SVMs) need pre-wired finite time
      windows to predict and classify time series. They do not have an internal
      state necessary to deal with sequences involving arbitrary long-term
      dependencies. Here we introduce a new class of recurrent, truly sequential
      SVM-like devices with internal adaptive states, trained by a novel method
      called EVOlution of systems with KErnel-based outputs (Evoke), an instance
      of the recent Evolino class of methods. Evoke evolves recurrent neural
      networks to detect and represent temporal dependencies while using
      quadratic programming/support vector regression to produce precise
      outputs. Evoke is the first SVM-based mechanism learning to classify a
      context-sensitive language. It also outperforms recent state-of-the-art
      gradient-based recurrent neural networks (RNNs) on various time series
      prediction tasks.
    title: Evolino for recurrent support vector machines
    updated: 2005-12-15T15:05:22.000Z
    url: 'http://arxiv.org/abs/cs/0512062v1'
    year: 2005
  - authors:
      - name: M. Botvinick
      - name: D. G. T. Barrett
      - name: P. Battaglia
      - name: N. de Freitas
      - name: D. Kumaran
      - name: J. Z Leibo
      - name: T. Lillicrap
      - name: J. Modayil
      - name: S. Mohamed
      - name: N. C. Rabinowitz
      - name: D. J. Rezende
      - name: A. Santoro
      - name: T. Schaul
      - name: C. Summerfield
      - name: G. Wayne
      - name: T. Weber
      - name: D. Wierstra
      - name: S. Legg
      - name: D. Hassabis
    categories:
      - cs.AI
    files:
      - 1711.08378v1.pdf
    id: 'http://arxiv.org/abs/1711.08378v1'
    links:
      - href: 'http://arxiv.org/abs/1711.08378v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1711.08378v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1711.08378v1'
    published: 2017-11-22T16:35:29.000Z
    summary: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
    title: >-
      Building Machines that Learn and Think for Themselves: Commentary on Lake
      et al., Behavioral and Brain Sciences, 2017
    updated: 2017-11-22T16:35:29.000Z
    url: 'http://arxiv.org/abs/1711.08378v1'
    year: 2017
  - authors:
      - name: Théophane Weber
      - name: Sébastien Racanière
      - name: David P. Reichert
      - name: Lars Buesing
      - name: Arthur Guez
      - name: Danilo Jimenez Rezende
      - name: Adria Puigdomènech Badia
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Yujia Li
      - name: Razvan Pascanu
      - name: Peter Battaglia
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
      - stat.ML
    files:
      - 1707.06203v1.pdf
    id: 'http://arxiv.org/abs/1707.06203v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06203v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06203v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06203v1'
    published: 2017-07-19T17:12:56.000Z
    summary: >-
      We introduce Imagination-Augmented Agents (I2As), a novel architecture for
      deep reinforcement learning combining model-free and model-based aspects.
      In contrast to most existing model-based reinforcement learning and
      planning methods, which prescribe how a model should be used to arrive at
      a policy, I2As learn to interpret predictions from a learned environment
      model to construct implicit plans in arbitrary ways, by using the
      predictions as additional context in deep policy networks. I2As show
      improved data efficiency, performance, and robustness to model
      misspecification compared to several baselines.
    title: Imagination-Augmented Agents for Deep Reinforcement Learning
    updated: 2017-07-19T17:12:56.000Z
    url: 'http://arxiv.org/abs/1707.06203v1'
    year: 2017
  - authors:
      - name: Razvan Pascanu
      - name: Yujia Li
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Lars Buesing
      - name: Sebastien Racanière
      - name: David Reichert
      - name: Théophane Weber
      - name: Daan Wierstra
      - name: Peter Battaglia
    categories:
      - cs.AI
      - cs.LG
      - cs.NE
      - stat.ML
    files:
      - 1707.06170v1.pdf
    id: 'http://arxiv.org/abs/1707.06170v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06170v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06170v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06170v1'
    published: 2017-07-19T15:52:35.000Z
    summary: >-
      Conventional wisdom holds that model-based planning is a powerful approach
      to sequential decision-making. It is often very challenging in practice,
      however, because while a model can be used to evaluate a plan, it does not
      prescribe how to construct a plan. Here we introduce the
      "Imagination-based Planner", the first model-based, sequential
      decision-making agent that can learn to construct, evaluate, and execute
      plans. Before any action, it can perform a variable number of imagination
      steps, which involve proposing an imagined action and evaluating it with
      its model-based imagination. All imagined actions and outcomes are
      aggregated, iteratively, into a "plan context" which conditions future
      real and imagined actions. The agent can even decide how to imagine:
      testing out alternative imagined actions, chaining sequences of actions
      together, or building a more complex "imagination tree" by navigating
      flexibly among the previously imagined states using a learned policy. And
      our agent can learn to plan economically, jointly optimizing for external
      rewards and computational costs associated with using its imagination. We
      show that our architecture can learn to solve a challenging continuous
      control problem, and also learn elaborate planning strategies in a
      discrete maze-solving task. Our work opens a new direction toward learning
      the components of a model-based planning system and how to use them.
    title: Learning model-based planning from scratch
    updated: 2017-07-19T15:52:35.000Z
    url: 'http://arxiv.org/abs/1707.06170v1'
    year: 2017
  - authors:
      - name: Ivo Danihelka
      - name: Balaji Lakshminarayanan
      - name: Benigno Uria
      - name: Daan Wierstra
      - name: Peter Dayan
    categories:
      - cs.LG
    files:
      - 1705.05263v1.pdf
    id: 'http://arxiv.org/abs/1705.05263v1'
    links:
      - href: 'http://arxiv.org/abs/1705.05263v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1705.05263v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1705.05263v1'
    published: 2017-05-15T14:24:01.000Z
    summary: >-
      We train a generator by maximum likelihood and we also train the same
      generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein
      distances. We show that an independent critic trained to approximate
      Wasserstein distance between the validation set and the generator
      distribution helps detect overfitting. Finally, we use ideas from the
      one-shot learning literature to develop a novel fast learning critic.
    title: Comparison of Maximum Likelihood and GAN-based training of Real NVPs
    updated: 2017-05-15T14:24:01.000Z
    url: 'http://arxiv.org/abs/1705.05263v1'
    year: 2017
  - authors:
      - name: Silvia Chiappa
      - name: Sébastien Racaniere
      - name: Daan Wierstra
      - name: Shakir Mohamed
    categories:
      - cs.AI
      - cs.LG
      - stat.ML
    files:
      - 1704.02254v2.pdf
    id: 'http://arxiv.org/abs/1704.02254v2'
    links:
      - href: 'http://arxiv.org/abs/1704.02254v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1704.02254v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1704.02254v2'
    published: 2017-04-07T14:53:54.000Z
    summary: >-
      Models that can simulate how environments change in response to actions
      can be used by agents to plan and act efficiently. We improve on previous
      environment simulators from high-dimensional pixel observations by
      introducing recurrent neural networks that are able to make temporally and
      spatially coherent predictions for hundreds of time-steps into the future.
      We present an in-depth analysis of the factors affecting performance,
      providing the most extensive attempt to advance the understanding of the
      properties of these models. We address the issue of computationally
      inefficiency with a model that does not need to generate a
      high-dimensional image at each time-step. We show that our approach can be
      used to improve exploration and is adaptable to many diverse environments,
      namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.
    title: Recurrent Environment Simulators
    updated: 2017-04-19T15:43:32.000Z
    url: 'http://arxiv.org/abs/1704.02254v2'
    year: 2017
  - authors:
      - name: Alexander Pritzel
      - name: Benigno Uria
      - name: Sriram Srinivasan
      - name: Adrià Puigdomènech
      - name: Oriol Vinyals
      - name: Demis Hassabis
      - name: Daan Wierstra
      - name: Charles Blundell
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1703.01988v1.pdf
    id: 'http://arxiv.org/abs/1703.01988v1'
    links:
      - href: 'http://arxiv.org/abs/1703.01988v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1703.01988v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1703.01988v1'
    published: 2017-03-06T17:23:27.000Z
    summary: >-
      Deep reinforcement learning methods attain super-human performance in a
      wide range of environments. Such methods are grossly inefficient, often
      taking orders of magnitudes more data than humans to achieve reasonable
      performance. We propose Neural Episodic Control: a deep reinforcement
      learning agent that is able to rapidly assimilate new experiences and act
      upon them. Our agent uses a semi-tabular representation of the value
      function: a buffer of past experience containing slowly changing state
      representations and rapidly updated estimates of the value function. We
      show across a wide range of environments that our agent learns
      significantly faster than other state-of-the-art, general purpose deep
      reinforcement learning agents.
    title: Neural Episodic Control
    updated: 2017-03-06T17:23:27.000Z
    url: 'http://arxiv.org/abs/1703.01988v1'
    year: 2017
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Charles Blundell
      - name: Yori Zwols
      - name: David Ha
      - name: Andrei A. Rusu
      - name: Alexander Pritzel
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.LG
    files:
      - 1701.08734v1.pdf
    id: 'http://arxiv.org/abs/1701.08734v1'
    links:
      - href: 'http://arxiv.org/abs/1701.08734v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1701.08734v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1701.08734v1'
    published: 2017-01-30T18:06:07.000Z
    summary: >-
      For artificial general intelligence (AGI) it would be efficient if
      multiple users trained the same giant neural network, permitting parameter
      reuse, without catastrophic forgetting. PathNet is a first step in this
      direction. It is a neural network algorithm that uses agents embedded in
      the neural network whose task is to discover which parts of the network to
      re-use for new tasks. Agents are pathways (views) through the network
      which determine the subset of parameters that are used and updated by the
      forwards and backwards passes of the backpropogation algorithm. During
      learning, a tournament selection genetic algorithm is used to select
      pathways through the neural network for replication and mutation. Pathway
      fitness is the performance of that pathway measured according to a cost
      function. We demonstrate successful transfer learning; fixing the
      parameters along a path learned on task A and re-evolving a new population
      of paths for task B, allows task B to be learned faster than it could be
      learned from scratch or after fine-tuning. Paths evolved on task B re-use
      parts of the optimal path evolved on task A. Positive transfer was
      demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
      classification tasks, and a set of Atari and Labyrinth reinforcement
      learning tasks, suggesting PathNets have general applicability for neural
      network training. Finally, PathNet also significantly improves the
      robustness to hyperparameter choices of a parallel asynchronous
      reinforcement learning algorithm (A3C).
    title: 'PathNet: Evolution Channels Gradient Descent in Super Neural Networks'
    updated: 2017-01-30T18:06:07.000Z
    url: 'http://arxiv.org/abs/1701.08734v1'
    year: 2017
  - authors:
      - name: Karol Gregor
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
    files:
      - 1611.07507v1.pdf
    id: 'http://arxiv.org/abs/1611.07507v1'
    links:
      - href: 'http://arxiv.org/abs/1611.07507v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1611.07507v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1611.07507v1'
    published: 2016-11-22T20:44:39.000Z
    summary: >-
      In this paper we introduce a new unsupervised reinforcement learning
      method for discovering the set of intrinsic options available to an agent.
      This set is learned by maximizing the number of different states an agent
      can reliably reach, as measured by the mutual information between the set
      of options and option termination states. To this end, we instantiate two
      policy gradient based algorithms, one that creates an explicit embedding
      space of options and one that represents options implicitly. The
      algorithms also provide an explicit measure of empowerment in a given
      state that can be used by an empowerment maximizing agent. The algorithm
      scales well with function approximation and we demonstrate the
      applicability of the algorithm on a range of tasks.
    title: Variational Intrinsic Control
    updated: 2016-11-22T20:44:39.000Z
    url: 'http://arxiv.org/abs/1611.07507v1'
    year: 2016
  - authors:
      - name: Charles Blundell
      - name: Benigno Uria
      - name: Alexander Pritzel
      - name: Yazhe Li
      - name: Avraham Ruderman
      - name: Joel Z Leibo
      - name: Jack Rae
      - name: Daan Wierstra
      - name: Demis Hassabis
    categories:
      - stat.ML
      - cs.LG
      - q-bio.NC
    files:
      - 1606.04460v1.pdf
    id: 'http://arxiv.org/abs/1606.04460v1'
    links:
      - href: 'http://arxiv.org/abs/1606.04460v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04460v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04460v1'
    published: 2016-06-14T17:03:46.000Z
    summary: >-
      State of the art deep reinforcement learning algorithms take many millions
      of interactions to attain human-level performance. Humans, on the other
      hand, can very quickly exploit highly rewarding nuances of an environment
      upon first discovery. In the brain, such rapid learning is thought to
      depend on the hippocampus and its capacity for episodic memory. Here we
      investigate whether a simple model of hippocampal episodic control can
      learn to solve difficult sequential decision-making tasks. We demonstrate
      that it not only attains a highly rewarding strategy significantly faster
      than state-of-the-art deep reinforcement learning algorithms, but also
      achieves a higher overall reward on some of the more challenging domains.
    title: Model-Free Episodic Control
    updated: 2016-06-14T17:03:46.000Z
    url: 'http://arxiv.org/abs/1606.04460v1'
    year: 2016
  - authors:
      - name: Oriol Vinyals
      - name: Charles Blundell
      - name: Timothy Lillicrap
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1606.04080v2.pdf
    id: 'http://arxiv.org/abs/1606.04080v2'
    links:
      - href: 'http://arxiv.org/abs/1606.04080v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04080v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04080v2'
    published: 2016-06-13T19:34:22.000Z
    summary: >-
      Learning from a few examples remains a key challenge in machine learning.
      Despite recent advances in important domains such as vision and language,
      the standard supervised deep learning paradigm does not offer a
      satisfactory solution for learning new concepts rapidly from little data.
      In this work, we employ ideas from metric learning based on deep neural
      features and from recent advances that augment neural networks with
      external memories. Our framework learns a network that maps a small
      labelled support set and an unlabelled example to its label, obviating the
      need for fine-tuning to adapt to new class types. We then define one-shot
      learning problems on vision (using Omniglot, ImageNet) and language tasks.
      Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2%
      and from 88.0% to 93.8% on Omniglot compared to competing approaches. We
      also demonstrate the usefulness of the same model on language modeling by
      introducing a one-shot task on the Penn Treebank.
    title: Matching Networks for One Shot Learning
    updated: 2017-12-29T17:45:19.000Z
    url: 'http://arxiv.org/abs/1606.04080v2'
    year: 2016
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Malcolm Reynolds
      - name: Frederic Besse
      - name: David Pfau
      - name: Max Jaderberg
      - name: Marc Lanctot
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.CV
      - cs.LG
    files:
      - 1606.02580v1.pdf
    id: 'http://arxiv.org/abs/1606.02580v1'
    links:
      - href: 'http://arxiv.org/abs/1606.02580v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.02580v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.02580v1'
    published: 2016-06-08T14:37:39.000Z
    summary: >-
      In this work we introduce a differentiable version of the Compositional
      Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the
      topology of a DPPN is evolved but the weights are learned. A Lamarckian
      algorithm, that combines evolution and learning, produces DPPNs to
      reconstruct an image. Our main result is that DPPNs can be evolved/trained
      to compress the weights of a denoising autoencoder from 157684 to roughly
      200 parameters, while achieving a reconstruction accuracy comparable to a
      fully connected network with more than two orders of magnitude more
      parameters. The regularization ability of the DPPN allows it to rediscover
      (approximate) convolutional network architectures embedded within a fully
      connected architecture. Such convolutional architectures are the current
      state of the art for many computer vision applications, so it is
      satisfying that DPPNs are capable of discovering this structure rather
      than having to build it in by design. DPPNs exhibit better generalization
      when tested on the Omniglot dataset after being trained on MNIST, than
      directly encoded fully connected autoencoders. DPPNs are therefore a new
      framework for integrating learning and evolution.
    title: 'Convolution by Evolution: Differentiable Pattern Producing Networks'
    updated: 2016-06-08T14:37:39.000Z
    url: 'http://arxiv.org/abs/1606.02580v1'
    year: 2016
  - authors:
      - name: Adam Santoro
      - name: Sergey Bartunov
      - name: Matthew Botvinick
      - name: Daan Wierstra
      - name: Timothy Lillicrap
    categories:
      - cs.LG
    files:
      - 1605.06065v1.pdf
    id: 'http://arxiv.org/abs/1605.06065v1'
    links:
      - href: 'http://arxiv.org/abs/1605.06065v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1605.06065v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1605.06065v1'
    published: 2016-05-19T17:44:51.000Z
    summary: >-
      Despite recent breakthroughs in the applications of deep neural networks,
      one setting that presents a persistent challenge is that of "one-shot
      learning." Traditional gradient-based networks require a lot of data to
      learn, often through extensive iterative training. When new data is
      encountered, the models must inefficiently relearn their parameters to
      adequately incorporate the new information without catastrophic
      interference. Architectures with augmented memory capacities, such as
      Neural Turing Machines (NTMs), offer the ability to quickly encode and
      retrieve new information, and hence can potentially obviate the downsides
      of conventional models. Here, we demonstrate the ability of a
      memory-augmented neural network to rapidly assimilate new data, and
      leverage this data to make accurate predictions after only a few samples.
      We also introduce a new method for accessing an external memory that
      focuses on memory content, unlike previous methods that additionally use
      memory location-based focusing mechanisms.
    title: One-shot Learning with Memory-Augmented Neural Networks
    updated: 2016-05-19T17:44:51.000Z
    url: 'http://arxiv.org/abs/1605.06065v1'
    year: 2016
  - authors:
      - name: Karol Gregor
      - name: Frederic Besse
      - name: Danilo Jimenez Rezende
      - name: Ivo Danihelka
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.CV
      - cs.LG
    files:
      - 1604.08772v1.pdf
    id: 'http://arxiv.org/abs/1604.08772v1'
    links:
      - href: 'http://arxiv.org/abs/1604.08772v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1604.08772v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1604.08772v1'
    published: 2016-04-29T11:02:52.000Z
    summary: >-
      We introduce a simple recurrent variational auto-encoder architecture that
      significantly improves image modeling. The system represents the
      state-of-the-art in latent variable models for both the ImageNet and
      Omniglot datasets. We show that it naturally separates global conceptual
      information from lower level details, thus addressing one of the
      fundamentally desired properties of unsupervised learning. Furthermore,
      the possibility of restricting ourselves to storing only global
      information about an image allows us to achieve high quality 'conceptual
      compression'.
    title: Towards Conceptual Compression
    updated: 2016-04-29T11:02:52.000Z
    url: 'http://arxiv.org/abs/1604.08772v1'
    year: 2016
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Ivo Danihelka
      - name: Karol Gregor
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
    files:
      - 1603.05106v2.pdf
    id: 'http://arxiv.org/abs/1603.05106v2'
    links:
      - href: 'http://arxiv.org/abs/1603.05106v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1603.05106v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1603.05106v2'
    published: 2016-03-16T14:10:00.000Z
    summary: >-
      Humans have an impressive ability to reason about new concepts and
      experiences from just a single example. In particular, humans have an
      ability for one-shot generalization: an ability to encounter a new
      concept, understand its structure, and then be able to generate compelling
      alternative variations of the concept. We develop machine learning systems
      with this important capacity by developing new deep generative models,
      models that combine the representational power of deep learning with the
      inferential power of Bayesian reasoning. We develop a class of sequential
      generative models that are built on the principles of feedback and
      attention. These two characteristics lead to generative models that are
      among the state-of-the art in density estimation and image generation. We
      demonstrate the one-shot generalization ability of our models using three
      tasks: unconditional sampling, generating new exemplars of a given
      concept, and generating new exemplars of a family of concepts. In all
      cases our models are able to generate compelling and diverse
      samples---having seen new examples just once---providing an important
      class of general-purpose models for one-shot machine learning.
    title: One-Shot Generalization in Deep Generative Models
    updated: 2016-05-25T12:57:19.000Z
    url: 'http://arxiv.org/abs/1603.05106v2'
    year: 2016
  - authors:
      - name: Timothy P. Lillicrap
      - name: Jonathan J. Hunt
      - name: Alexander Pritzel
      - name: Nicolas Heess
      - name: Tom Erez
      - name: Yuval Tassa
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1509.02971v5.pdf
    id: 'http://arxiv.org/abs/1509.02971v5'
    links:
      - href: 'http://arxiv.org/abs/1509.02971v5'
        title: ''
      - href: 'http://arxiv.org/pdf/1509.02971v5'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1509.02971v5'
    published: 2015-09-09T23:01:36.000Z
    summary: >-
      We adapt the ideas underlying the success of Deep Q-Learning to the
      continuous action domain. We present an actor-critic, model-free algorithm
      based on the deterministic policy gradient that can operate over
      continuous action spaces. Using the same learning algorithm, network
      architecture and hyper-parameters, our algorithm robustly solves more than
      20 simulated physics tasks, including classic problems such as cartpole
      swing-up, dexterous manipulation, legged locomotion and car driving. Our
      algorithm is able to find policies whose performance is competitive with
      those found by a planning algorithm with full access to the dynamics of
      the domain and its derivatives. We further demonstrate that for many of
      the tasks the algorithm can learn policies end-to-end: directly from raw
      pixel inputs.
    title: Continuous control with deep reinforcement learning
    updated: 2016-02-29T18:45:53.000Z
    url: 'http://arxiv.org/abs/1509.02971v5'
    year: 2015
  - authors:
      - name: Charles Blundell
      - name: Julien Cornebise
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.LG
    files:
      - 1505.05424v2.pdf
    id: 'http://arxiv.org/abs/1505.05424v2'
    links:
      - href: 'http://arxiv.org/abs/1505.05424v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1505.05424v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1505.05424v2'
    published: 2015-05-20T15:39:48.000Z
    summary: >-
      We introduce a new, efficient, principled and backpropagation-compatible
      algorithm for learning a probability distribution on the weights of a
      neural network, called Bayes by Backprop. It regularises the weights by
      minimising a compression cost, known as the variational free energy or the
      expected lower bound on the marginal likelihood. We show that this
      principled kind of regularisation yields comparable performance to dropout
      on MNIST classification. We then demonstrate how the learnt uncertainty in
      the weights can be used to improve generalisation in non-linear regression
      problems, and how this weight uncertainty can be used to drive the
      exploration-exploitation trade-off in reinforcement learning.
    title: Weight Uncertainty in Neural Networks
    updated: 2015-05-21T14:07:23.000Z
    url: 'http://arxiv.org/abs/1505.05424v2'
    year: 2015
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Alex Graves
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.CV
      - cs.LG
      - cs.NE
    files:
      - 1502.04623v2.pdf
    id: 'http://arxiv.org/abs/1502.04623v2'
    links:
      - href: 'http://arxiv.org/abs/1502.04623v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1502.04623v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1502.04623v2'
    published: 2015-02-16T16:48:56.000Z
    summary: >-
      This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
      network architecture for image generation. DRAW networks combine a novel
      spatial attention mechanism that mimics the foveation of the human eye,
      with a sequential variational auto-encoding framework that allows for the
      iterative construction of complex images. The system substantially
      improves on the state of the art for generative models on MNIST, and, when
      trained on the Street View House Numbers dataset, it generates images that
      cannot be distinguished from real data with the naked eye.
    title: 'DRAW: A Recurrent Neural Network For Image Generation'
    updated: 2015-05-20T15:29:42.000Z
    url: 'http://arxiv.org/abs/1502.04623v2'
    year: 2015
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
      - stat.CO
      - stat.ME
    files:
      - 1401.4082v3.pdf
    id: 'http://arxiv.org/abs/1401.4082v3'
    links:
      - href: 'http://arxiv.org/abs/1401.4082v3'
        title: ''
      - href: 'http://arxiv.org/pdf/1401.4082v3'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1401.4082v3'
    published: 2014-01-16T16:33:23.000Z
    summary: >-
      We marry ideas from deep neural networks and approximate Bayesian
      inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning.
      Our algorithm introduces a recognition model to represent approximate
      posterior distributions, and that acts as a stochastic encoder of the
      data. We develop stochastic back-propagation -- rules for back-propagation
      through stochastic variables -- and use this to develop an algorithm that
      allows for joint optimisation of the parameters of both the generative and
      recognition model. We demonstrate on several real-world data sets that the
      model generates realistic samples, provides accurate imputations of
      missing data and is a useful tool for high-dimensional data visualisation.
    title: >-
      Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models
    updated: 2014-05-30T10:00:36.000Z
    url: 'http://arxiv.org/abs/1401.4082v3'
    year: 2014
  - authors:
      - name: Volodymyr Mnih
      - name: Koray Kavukcuoglu
      - name: David Silver
      - name: Alex Graves
      - name: Ioannis Antonoglou
      - name: Daan Wierstra
      - name: Martin Riedmiller
    categories:
      - cs.LG
    files:
      - 1312.5602v1.pdf
    id: 'http://arxiv.org/abs/1312.5602v1'
    links:
      - href: 'http://arxiv.org/abs/1312.5602v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1312.5602v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1312.5602v1'
    published: 2013-12-19T16:00:08.000Z
    summary: >-
      We present the first deep learning model to successfully learn control
      policies directly from high-dimensional sensory input using reinforcement
      learning. The model is a convolutional neural network, trained with a
      variant of Q-learning, whose input is raw pixels and whose output is a
      value function estimating future rewards. We apply our method to seven
      Atari 2600 games from the Arcade Learning Environment, with no adjustment
      of the architecture or learning algorithm. We find that it outperforms all
      previous approaches on six of the games and surpasses a human expert on
      three of them.
    title: Playing Atari with Deep Reinforcement Learning
    updated: 2013-12-19T16:00:08.000Z
    url: 'http://arxiv.org/abs/1312.5602v1'
    year: 2013
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Andriy Mnih
      - name: Charles Blundell
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1310.8499v2.pdf
    id: 'http://arxiv.org/abs/1310.8499v2'
    links:
      - href: 'http://arxiv.org/abs/1310.8499v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1310.8499v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1310.8499v2'
    published: 2013-10-31T13:47:30.000Z
    summary: >-
      We introduce a deep, generative autoencoder capable of learning
      hierarchies of distributed representations from data. Successive deep
      stochastic hidden layers are equipped with autoregressive connections,
      which enable the model to be sampled from quickly and exactly via
      ancestral sampling. We derive an efficient approximate parameter
      estimation method based on the minimum description length (MDL) principle,
      which can be seen as maximising a variational lower bound on the
      log-likelihood, with a feedforward neural network implementing approximate
      inference. We demonstrate state-of-the-art generative performance on a
      number of classic data sets: several UCI data sets, MNIST and Atari 2600
      games.
    title: Deep AutoRegressive Networks
    updated: 2014-05-20T16:22:43.000Z
    url: 'http://arxiv.org/abs/1310.8499v2'
    year: 2013
  - authors:
      - name: Yi Sun
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Juergen Schmidhuber
    categories:
      - cs.AI
    files:
      - 1209.5853v1.pdf
    id: 'http://arxiv.org/abs/1209.5853v1'
    links:
      - href: 'http://arxiv.org/abs/1209.5853v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1209.5853v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1209.5853v1'
    published: 2012-09-26T07:42:06.000Z
    summary: >-
      Efficient Natural Evolution Strategies (eNES) is a novel alternative to
      conventional evolutionary algorithms, using the natural gradient to adapt
      the mutation distribution. Unlike previous methods based on natural
      gradients, eNES uses a fast algorithm to calculate the inverse of the
      exact Fisher information matrix, thus increasing both robustness and
      performance of its evolution gradient estimation, even in higher
      dimensions. Additional novel aspects of eNES include optimal fitness
      baselines and importance mixing (a procedure for updating the population
      with very few fitness evaluations). The algorithm yields competitive
      results on both unimodal and multimodal benchmarks.
    title: Efficient Natural Evolution Strategies
    updated: 2012-09-26T07:42:06.000Z
    url: 'http://arxiv.org/abs/1209.5853v1'
    year: 2012
  - authors:
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Tobias Glasmachers
      - name: Yi Sun
      - name: Jürgen Schmidhuber
    categories:
      - stat.ML
      - cs.NE
    files:
      - 1106.4487v1.pdf
    id: 'http://arxiv.org/abs/1106.4487v1'
    links:
      - href: 'http://arxiv.org/abs/1106.4487v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1106.4487v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1106.4487v1'
    published: 2011-06-22T15:55:52.000Z
    summary: >-
      This paper presents Natural Evolution Strategies (NES), a recent family of
      algorithms that constitute a more principled approach to black-box
      optimization than established evolutionary algorithms. NES maintains a
      parameterized distribution on the set of solution candidates, and the
      natural gradient is used to update the distribution's parameters in the
      direction of higher expected fitness. We introduce a collection of
      techniques that address issues of convergence, robustness, sample
      complexity, computational complexity and sensitivity to hyperparameters.
      This paper explores a number of implementations of the NES family, ranging
      from general-purpose multi-variate normal distributions to heavy-tailed
      and separable distributions tailored towards global optimization and
      search in high dimensional spaces, respectively. Experimental results show
      best published performance on various standard benchmarks, as well as
      competitive performance on others.
    title: Natural Evolution Strategies
    updated: 2011-06-22T15:55:52.000Z
    url: 'http://arxiv.org/abs/1106.4487v1'
    year: 2011
  - authors:
      - name: Juergen Schmidhuber
      - name: Matteo Gagliolo
      - name: Daan Wierstra
      - name: Faustino Gomez
    categories:
      - cs.NE
      - F.1.1; I.2.6
    files:
      - 0512062v1.pdf
    id: 'http://arxiv.org/abs/cs/0512062v1'
    links:
      - href: 'http://arxiv.org/abs/cs/0512062v1'
        title: ''
      - href: 'http://arxiv.org/pdf/cs/0512062v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/cs/0512062v1'
    published: 2005-12-15T15:05:22.000Z
    summary: >-
      Traditional Support Vector Machines (SVMs) need pre-wired finite time
      windows to predict and classify time series. They do not have an internal
      state necessary to deal with sequences involving arbitrary long-term
      dependencies. Here we introduce a new class of recurrent, truly sequential
      SVM-like devices with internal adaptive states, trained by a novel method
      called EVOlution of systems with KErnel-based outputs (Evoke), an instance
      of the recent Evolino class of methods. Evoke evolves recurrent neural
      networks to detect and represent temporal dependencies while using
      quadratic programming/support vector regression to produce precise
      outputs. Evoke is the first SVM-based mechanism learning to classify a
      context-sensitive language. It also outperforms recent state-of-the-art
      gradient-based recurrent neural networks (RNNs) on various time series
      prediction tasks.
    title: Evolino for recurrent support vector machines
    updated: 2005-12-15T15:05:22.000Z
    url: 'http://arxiv.org/abs/cs/0512062v1'
    year: 2005
  - authors:
      - name: M. Botvinick
      - name: D. G. T. Barrett
      - name: P. Battaglia
      - name: N. de Freitas
      - name: D. Kumaran
      - name: J. Z Leibo
      - name: T. Lillicrap
      - name: J. Modayil
      - name: S. Mohamed
      - name: N. C. Rabinowitz
      - name: D. J. Rezende
      - name: A. Santoro
      - name: T. Schaul
      - name: C. Summerfield
      - name: G. Wayne
      - name: T. Weber
      - name: D. Wierstra
      - name: S. Legg
      - name: D. Hassabis
    categories:
      - cs.AI
    files:
      - 1711.08378v1.pdf
    id: 'http://arxiv.org/abs/1711.08378v1'
    links:
      - href: 'http://arxiv.org/abs/1711.08378v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1711.08378v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1711.08378v1'
    published: 2017-11-22T16:35:29.000Z
    summary: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
    title: >-
      Building Machines that Learn and Think for Themselves: Commentary on Lake
      et al., Behavioral and Brain Sciences, 2017
    updated: 2017-11-22T16:35:29.000Z
    url: 'http://arxiv.org/abs/1711.08378v1'
    year: 2017
  - authors:
      - name: Théophane Weber
      - name: Sébastien Racanière
      - name: David P. Reichert
      - name: Lars Buesing
      - name: Arthur Guez
      - name: Danilo Jimenez Rezende
      - name: Adria Puigdomènech Badia
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Yujia Li
      - name: Razvan Pascanu
      - name: Peter Battaglia
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
      - stat.ML
    files:
      - 1707.06203v1.pdf
    id: 'http://arxiv.org/abs/1707.06203v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06203v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06203v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06203v1'
    published: 2017-07-19T17:12:56.000Z
    summary: >-
      We introduce Imagination-Augmented Agents (I2As), a novel architecture for
      deep reinforcement learning combining model-free and model-based aspects.
      In contrast to most existing model-based reinforcement learning and
      planning methods, which prescribe how a model should be used to arrive at
      a policy, I2As learn to interpret predictions from a learned environment
      model to construct implicit plans in arbitrary ways, by using the
      predictions as additional context in deep policy networks. I2As show
      improved data efficiency, performance, and robustness to model
      misspecification compared to several baselines.
    title: Imagination-Augmented Agents for Deep Reinforcement Learning
    updated: 2017-07-19T17:12:56.000Z
    url: 'http://arxiv.org/abs/1707.06203v1'
    year: 2017
  - authors:
      - name: Razvan Pascanu
      - name: Yujia Li
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Lars Buesing
      - name: Sebastien Racanière
      - name: David Reichert
      - name: Théophane Weber
      - name: Daan Wierstra
      - name: Peter Battaglia
    categories:
      - cs.AI
      - cs.LG
      - cs.NE
      - stat.ML
    files:
      - 1707.06170v1.pdf
    id: 'http://arxiv.org/abs/1707.06170v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06170v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06170v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06170v1'
    published: 2017-07-19T15:52:35.000Z
    summary: >-
      Conventional wisdom holds that model-based planning is a powerful approach
      to sequential decision-making. It is often very challenging in practice,
      however, because while a model can be used to evaluate a plan, it does not
      prescribe how to construct a plan. Here we introduce the
      "Imagination-based Planner", the first model-based, sequential
      decision-making agent that can learn to construct, evaluate, and execute
      plans. Before any action, it can perform a variable number of imagination
      steps, which involve proposing an imagined action and evaluating it with
      its model-based imagination. All imagined actions and outcomes are
      aggregated, iteratively, into a "plan context" which conditions future
      real and imagined actions. The agent can even decide how to imagine:
      testing out alternative imagined actions, chaining sequences of actions
      together, or building a more complex "imagination tree" by navigating
      flexibly among the previously imagined states using a learned policy. And
      our agent can learn to plan economically, jointly optimizing for external
      rewards and computational costs associated with using its imagination. We
      show that our architecture can learn to solve a challenging continuous
      control problem, and also learn elaborate planning strategies in a
      discrete maze-solving task. Our work opens a new direction toward learning
      the components of a model-based planning system and how to use them.
    title: Learning model-based planning from scratch
    updated: 2017-07-19T15:52:35.000Z
    url: 'http://arxiv.org/abs/1707.06170v1'
    year: 2017
  - authors:
      - name: Ivo Danihelka
      - name: Balaji Lakshminarayanan
      - name: Benigno Uria
      - name: Daan Wierstra
      - name: Peter Dayan
    categories:
      - cs.LG
    files:
      - 1705.05263v1.pdf
    id: 'http://arxiv.org/abs/1705.05263v1'
    links:
      - href: 'http://arxiv.org/abs/1705.05263v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1705.05263v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1705.05263v1'
    published: 2017-05-15T14:24:01.000Z
    summary: >-
      We train a generator by maximum likelihood and we also train the same
      generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein
      distances. We show that an independent critic trained to approximate
      Wasserstein distance between the validation set and the generator
      distribution helps detect overfitting. Finally, we use ideas from the
      one-shot learning literature to develop a novel fast learning critic.
    title: Comparison of Maximum Likelihood and GAN-based training of Real NVPs
    updated: 2017-05-15T14:24:01.000Z
    url: 'http://arxiv.org/abs/1705.05263v1'
    year: 2017
  - authors:
      - name: Silvia Chiappa
      - name: Sébastien Racaniere
      - name: Daan Wierstra
      - name: Shakir Mohamed
    categories:
      - cs.AI
      - cs.LG
      - stat.ML
    files:
      - 1704.02254v2.pdf
    id: 'http://arxiv.org/abs/1704.02254v2'
    links:
      - href: 'http://arxiv.org/abs/1704.02254v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1704.02254v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1704.02254v2'
    published: 2017-04-07T14:53:54.000Z
    summary: >-
      Models that can simulate how environments change in response to actions
      can be used by agents to plan and act efficiently. We improve on previous
      environment simulators from high-dimensional pixel observations by
      introducing recurrent neural networks that are able to make temporally and
      spatially coherent predictions for hundreds of time-steps into the future.
      We present an in-depth analysis of the factors affecting performance,
      providing the most extensive attempt to advance the understanding of the
      properties of these models. We address the issue of computationally
      inefficiency with a model that does not need to generate a
      high-dimensional image at each time-step. We show that our approach can be
      used to improve exploration and is adaptable to many diverse environments,
      namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.
    title: Recurrent Environment Simulators
    updated: 2017-04-19T15:43:32.000Z
    url: 'http://arxiv.org/abs/1704.02254v2'
    year: 2017
  - authors:
      - name: Alexander Pritzel
      - name: Benigno Uria
      - name: Sriram Srinivasan
      - name: Adrià Puigdomènech
      - name: Oriol Vinyals
      - name: Demis Hassabis
      - name: Daan Wierstra
      - name: Charles Blundell
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1703.01988v1.pdf
    id: 'http://arxiv.org/abs/1703.01988v1'
    links:
      - href: 'http://arxiv.org/abs/1703.01988v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1703.01988v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1703.01988v1'
    published: 2017-03-06T17:23:27.000Z
    summary: >-
      Deep reinforcement learning methods attain super-human performance in a
      wide range of environments. Such methods are grossly inefficient, often
      taking orders of magnitudes more data than humans to achieve reasonable
      performance. We propose Neural Episodic Control: a deep reinforcement
      learning agent that is able to rapidly assimilate new experiences and act
      upon them. Our agent uses a semi-tabular representation of the value
      function: a buffer of past experience containing slowly changing state
      representations and rapidly updated estimates of the value function. We
      show across a wide range of environments that our agent learns
      significantly faster than other state-of-the-art, general purpose deep
      reinforcement learning agents.
    title: Neural Episodic Control
    updated: 2017-03-06T17:23:27.000Z
    url: 'http://arxiv.org/abs/1703.01988v1'
    year: 2017
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Charles Blundell
      - name: Yori Zwols
      - name: David Ha
      - name: Andrei A. Rusu
      - name: Alexander Pritzel
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.LG
    files:
      - 1701.08734v1.pdf
    id: 'http://arxiv.org/abs/1701.08734v1'
    links:
      - href: 'http://arxiv.org/abs/1701.08734v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1701.08734v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1701.08734v1'
    published: 2017-01-30T18:06:07.000Z
    summary: >-
      For artificial general intelligence (AGI) it would be efficient if
      multiple users trained the same giant neural network, permitting parameter
      reuse, without catastrophic forgetting. PathNet is a first step in this
      direction. It is a neural network algorithm that uses agents embedded in
      the neural network whose task is to discover which parts of the network to
      re-use for new tasks. Agents are pathways (views) through the network
      which determine the subset of parameters that are used and updated by the
      forwards and backwards passes of the backpropogation algorithm. During
      learning, a tournament selection genetic algorithm is used to select
      pathways through the neural network for replication and mutation. Pathway
      fitness is the performance of that pathway measured according to a cost
      function. We demonstrate successful transfer learning; fixing the
      parameters along a path learned on task A and re-evolving a new population
      of paths for task B, allows task B to be learned faster than it could be
      learned from scratch or after fine-tuning. Paths evolved on task B re-use
      parts of the optimal path evolved on task A. Positive transfer was
      demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
      classification tasks, and a set of Atari and Labyrinth reinforcement
      learning tasks, suggesting PathNets have general applicability for neural
      network training. Finally, PathNet also significantly improves the
      robustness to hyperparameter choices of a parallel asynchronous
      reinforcement learning algorithm (A3C).
    title: 'PathNet: Evolution Channels Gradient Descent in Super Neural Networks'
    updated: 2017-01-30T18:06:07.000Z
    url: 'http://arxiv.org/abs/1701.08734v1'
    year: 2017
  - authors:
      - name: Karol Gregor
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
    files:
      - 1611.07507v1.pdf
    id: 'http://arxiv.org/abs/1611.07507v1'
    links:
      - href: 'http://arxiv.org/abs/1611.07507v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1611.07507v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1611.07507v1'
    published: 2016-11-22T20:44:39.000Z
    summary: >-
      In this paper we introduce a new unsupervised reinforcement learning
      method for discovering the set of intrinsic options available to an agent.
      This set is learned by maximizing the number of different states an agent
      can reliably reach, as measured by the mutual information between the set
      of options and option termination states. To this end, we instantiate two
      policy gradient based algorithms, one that creates an explicit embedding
      space of options and one that represents options implicitly. The
      algorithms also provide an explicit measure of empowerment in a given
      state that can be used by an empowerment maximizing agent. The algorithm
      scales well with function approximation and we demonstrate the
      applicability of the algorithm on a range of tasks.
    title: Variational Intrinsic Control
    updated: 2016-11-22T20:44:39.000Z
    url: 'http://arxiv.org/abs/1611.07507v1'
    year: 2016
  - authors:
      - name: Charles Blundell
      - name: Benigno Uria
      - name: Alexander Pritzel
      - name: Yazhe Li
      - name: Avraham Ruderman
      - name: Joel Z Leibo
      - name: Jack Rae
      - name: Daan Wierstra
      - name: Demis Hassabis
    categories:
      - stat.ML
      - cs.LG
      - q-bio.NC
    files:
      - 1606.04460v1.pdf
    id: 'http://arxiv.org/abs/1606.04460v1'
    links:
      - href: 'http://arxiv.org/abs/1606.04460v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04460v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04460v1'
    published: 2016-06-14T17:03:46.000Z
    summary: >-
      State of the art deep reinforcement learning algorithms take many millions
      of interactions to attain human-level performance. Humans, on the other
      hand, can very quickly exploit highly rewarding nuances of an environment
      upon first discovery. In the brain, such rapid learning is thought to
      depend on the hippocampus and its capacity for episodic memory. Here we
      investigate whether a simple model of hippocampal episodic control can
      learn to solve difficult sequential decision-making tasks. We demonstrate
      that it not only attains a highly rewarding strategy significantly faster
      than state-of-the-art deep reinforcement learning algorithms, but also
      achieves a higher overall reward on some of the more challenging domains.
    title: Model-Free Episodic Control
    updated: 2016-06-14T17:03:46.000Z
    url: 'http://arxiv.org/abs/1606.04460v1'
    year: 2016
  - authors:
      - name: Oriol Vinyals
      - name: Charles Blundell
      - name: Timothy Lillicrap
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1606.04080v2.pdf
    id: 'http://arxiv.org/abs/1606.04080v2'
    links:
      - href: 'http://arxiv.org/abs/1606.04080v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04080v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04080v2'
    published: 2016-06-13T19:34:22.000Z
    summary: >-
      Learning from a few examples remains a key challenge in machine learning.
      Despite recent advances in important domains such as vision and language,
      the standard supervised deep learning paradigm does not offer a
      satisfactory solution for learning new concepts rapidly from little data.
      In this work, we employ ideas from metric learning based on deep neural
      features and from recent advances that augment neural networks with
      external memories. Our framework learns a network that maps a small
      labelled support set and an unlabelled example to its label, obviating the
      need for fine-tuning to adapt to new class types. We then define one-shot
      learning problems on vision (using Omniglot, ImageNet) and language tasks.
      Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2%
      and from 88.0% to 93.8% on Omniglot compared to competing approaches. We
      also demonstrate the usefulness of the same model on language modeling by
      introducing a one-shot task on the Penn Treebank.
    title: Matching Networks for One Shot Learning
    updated: 2017-12-29T17:45:19.000Z
    url: 'http://arxiv.org/abs/1606.04080v2'
    year: 2016
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Malcolm Reynolds
      - name: Frederic Besse
      - name: David Pfau
      - name: Max Jaderberg
      - name: Marc Lanctot
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.CV
      - cs.LG
    files:
      - 1606.02580v1.pdf
    id: 'http://arxiv.org/abs/1606.02580v1'
    links:
      - href: 'http://arxiv.org/abs/1606.02580v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.02580v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.02580v1'
    published: 2016-06-08T14:37:39.000Z
    summary: >-
      In this work we introduce a differentiable version of the Compositional
      Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the
      topology of a DPPN is evolved but the weights are learned. A Lamarckian
      algorithm, that combines evolution and learning, produces DPPNs to
      reconstruct an image. Our main result is that DPPNs can be evolved/trained
      to compress the weights of a denoising autoencoder from 157684 to roughly
      200 parameters, while achieving a reconstruction accuracy comparable to a
      fully connected network with more than two orders of magnitude more
      parameters. The regularization ability of the DPPN allows it to rediscover
      (approximate) convolutional network architectures embedded within a fully
      connected architecture. Such convolutional architectures are the current
      state of the art for many computer vision applications, so it is
      satisfying that DPPNs are capable of discovering this structure rather
      than having to build it in by design. DPPNs exhibit better generalization
      when tested on the Omniglot dataset after being trained on MNIST, than
      directly encoded fully connected autoencoders. DPPNs are therefore a new
      framework for integrating learning and evolution.
    title: 'Convolution by Evolution: Differentiable Pattern Producing Networks'
    updated: 2016-06-08T14:37:39.000Z
    url: 'http://arxiv.org/abs/1606.02580v1'
    year: 2016
  - authors:
      - name: Adam Santoro
      - name: Sergey Bartunov
      - name: Matthew Botvinick
      - name: Daan Wierstra
      - name: Timothy Lillicrap
    categories:
      - cs.LG
    files:
      - 1605.06065v1.pdf
    id: 'http://arxiv.org/abs/1605.06065v1'
    links:
      - href: 'http://arxiv.org/abs/1605.06065v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1605.06065v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1605.06065v1'
    published: 2016-05-19T17:44:51.000Z
    summary: >-
      Despite recent breakthroughs in the applications of deep neural networks,
      one setting that presents a persistent challenge is that of "one-shot
      learning." Traditional gradient-based networks require a lot of data to
      learn, often through extensive iterative training. When new data is
      encountered, the models must inefficiently relearn their parameters to
      adequately incorporate the new information without catastrophic
      interference. Architectures with augmented memory capacities, such as
      Neural Turing Machines (NTMs), offer the ability to quickly encode and
      retrieve new information, and hence can potentially obviate the downsides
      of conventional models. Here, we demonstrate the ability of a
      memory-augmented neural network to rapidly assimilate new data, and
      leverage this data to make accurate predictions after only a few samples.
      We also introduce a new method for accessing an external memory that
      focuses on memory content, unlike previous methods that additionally use
      memory location-based focusing mechanisms.
    title: One-shot Learning with Memory-Augmented Neural Networks
    updated: 2016-05-19T17:44:51.000Z
    url: 'http://arxiv.org/abs/1605.06065v1'
    year: 2016
  - authors:
      - name: Karol Gregor
      - name: Frederic Besse
      - name: Danilo Jimenez Rezende
      - name: Ivo Danihelka
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.CV
      - cs.LG
    files:
      - 1604.08772v1.pdf
    id: 'http://arxiv.org/abs/1604.08772v1'
    links:
      - href: 'http://arxiv.org/abs/1604.08772v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1604.08772v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1604.08772v1'
    published: 2016-04-29T11:02:52.000Z
    summary: >-
      We introduce a simple recurrent variational auto-encoder architecture that
      significantly improves image modeling. The system represents the
      state-of-the-art in latent variable models for both the ImageNet and
      Omniglot datasets. We show that it naturally separates global conceptual
      information from lower level details, thus addressing one of the
      fundamentally desired properties of unsupervised learning. Furthermore,
      the possibility of restricting ourselves to storing only global
      information about an image allows us to achieve high quality 'conceptual
      compression'.
    title: Towards Conceptual Compression
    updated: 2016-04-29T11:02:52.000Z
    url: 'http://arxiv.org/abs/1604.08772v1'
    year: 2016
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Ivo Danihelka
      - name: Karol Gregor
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
    files:
      - 1603.05106v2.pdf
    id: 'http://arxiv.org/abs/1603.05106v2'
    links:
      - href: 'http://arxiv.org/abs/1603.05106v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1603.05106v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1603.05106v2'
    published: 2016-03-16T14:10:00.000Z
    summary: >-
      Humans have an impressive ability to reason about new concepts and
      experiences from just a single example. In particular, humans have an
      ability for one-shot generalization: an ability to encounter a new
      concept, understand its structure, and then be able to generate compelling
      alternative variations of the concept. We develop machine learning systems
      with this important capacity by developing new deep generative models,
      models that combine the representational power of deep learning with the
      inferential power of Bayesian reasoning. We develop a class of sequential
      generative models that are built on the principles of feedback and
      attention. These two characteristics lead to generative models that are
      among the state-of-the art in density estimation and image generation. We
      demonstrate the one-shot generalization ability of our models using three
      tasks: unconditional sampling, generating new exemplars of a given
      concept, and generating new exemplars of a family of concepts. In all
      cases our models are able to generate compelling and diverse
      samples---having seen new examples just once---providing an important
      class of general-purpose models for one-shot machine learning.
    title: One-Shot Generalization in Deep Generative Models
    updated: 2016-05-25T12:57:19.000Z
    url: 'http://arxiv.org/abs/1603.05106v2'
    year: 2016
  - authors:
      - name: Timothy P. Lillicrap
      - name: Jonathan J. Hunt
      - name: Alexander Pritzel
      - name: Nicolas Heess
      - name: Tom Erez
      - name: Yuval Tassa
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1509.02971v5.pdf
    id: 'http://arxiv.org/abs/1509.02971v5'
    links:
      - href: 'http://arxiv.org/abs/1509.02971v5'
        title: ''
      - href: 'http://arxiv.org/pdf/1509.02971v5'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1509.02971v5'
    published: 2015-09-09T23:01:36.000Z
    summary: >-
      We adapt the ideas underlying the success of Deep Q-Learning to the
      continuous action domain. We present an actor-critic, model-free algorithm
      based on the deterministic policy gradient that can operate over
      continuous action spaces. Using the same learning algorithm, network
      architecture and hyper-parameters, our algorithm robustly solves more than
      20 simulated physics tasks, including classic problems such as cartpole
      swing-up, dexterous manipulation, legged locomotion and car driving. Our
      algorithm is able to find policies whose performance is competitive with
      those found by a planning algorithm with full access to the dynamics of
      the domain and its derivatives. We further demonstrate that for many of
      the tasks the algorithm can learn policies end-to-end: directly from raw
      pixel inputs.
    title: Continuous control with deep reinforcement learning
    updated: 2016-02-29T18:45:53.000Z
    url: 'http://arxiv.org/abs/1509.02971v5'
    year: 2015
  - authors:
      - name: Charles Blundell
      - name: Julien Cornebise
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.LG
    files:
      - 1505.05424v2.pdf
    id: 'http://arxiv.org/abs/1505.05424v2'
    links:
      - href: 'http://arxiv.org/abs/1505.05424v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1505.05424v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1505.05424v2'
    published: 2015-05-20T15:39:48.000Z
    summary: >-
      We introduce a new, efficient, principled and backpropagation-compatible
      algorithm for learning a probability distribution on the weights of a
      neural network, called Bayes by Backprop. It regularises the weights by
      minimising a compression cost, known as the variational free energy or the
      expected lower bound on the marginal likelihood. We show that this
      principled kind of regularisation yields comparable performance to dropout
      on MNIST classification. We then demonstrate how the learnt uncertainty in
      the weights can be used to improve generalisation in non-linear regression
      problems, and how this weight uncertainty can be used to drive the
      exploration-exploitation trade-off in reinforcement learning.
    title: Weight Uncertainty in Neural Networks
    updated: 2015-05-21T14:07:23.000Z
    url: 'http://arxiv.org/abs/1505.05424v2'
    year: 2015
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Alex Graves
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.CV
      - cs.LG
      - cs.NE
    files:
      - 1502.04623v2.pdf
    id: 'http://arxiv.org/abs/1502.04623v2'
    links:
      - href: 'http://arxiv.org/abs/1502.04623v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1502.04623v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1502.04623v2'
    published: 2015-02-16T16:48:56.000Z
    summary: >-
      This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
      network architecture for image generation. DRAW networks combine a novel
      spatial attention mechanism that mimics the foveation of the human eye,
      with a sequential variational auto-encoding framework that allows for the
      iterative construction of complex images. The system substantially
      improves on the state of the art for generative models on MNIST, and, when
      trained on the Street View House Numbers dataset, it generates images that
      cannot be distinguished from real data with the naked eye.
    title: 'DRAW: A Recurrent Neural Network For Image Generation'
    updated: 2015-05-20T15:29:42.000Z
    url: 'http://arxiv.org/abs/1502.04623v2'
    year: 2015
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
      - stat.CO
      - stat.ME
    files:
      - 1401.4082v3.pdf
    id: 'http://arxiv.org/abs/1401.4082v3'
    links:
      - href: 'http://arxiv.org/abs/1401.4082v3'
        title: ''
      - href: 'http://arxiv.org/pdf/1401.4082v3'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1401.4082v3'
    published: 2014-01-16T16:33:23.000Z
    summary: >-
      We marry ideas from deep neural networks and approximate Bayesian
      inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning.
      Our algorithm introduces a recognition model to represent approximate
      posterior distributions, and that acts as a stochastic encoder of the
      data. We develop stochastic back-propagation -- rules for back-propagation
      through stochastic variables -- and use this to develop an algorithm that
      allows for joint optimisation of the parameters of both the generative and
      recognition model. We demonstrate on several real-world data sets that the
      model generates realistic samples, provides accurate imputations of
      missing data and is a useful tool for high-dimensional data visualisation.
    title: >-
      Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models
    updated: 2014-05-30T10:00:36.000Z
    url: 'http://arxiv.org/abs/1401.4082v3'
    year: 2014
  - authors:
      - name: Volodymyr Mnih
      - name: Koray Kavukcuoglu
      - name: David Silver
      - name: Alex Graves
      - name: Ioannis Antonoglou
      - name: Daan Wierstra
      - name: Martin Riedmiller
    categories:
      - cs.LG
    files:
      - 1312.5602v1.pdf
    id: 'http://arxiv.org/abs/1312.5602v1'
    links:
      - href: 'http://arxiv.org/abs/1312.5602v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1312.5602v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1312.5602v1'
    published: 2013-12-19T16:00:08.000Z
    summary: >-
      We present the first deep learning model to successfully learn control
      policies directly from high-dimensional sensory input using reinforcement
      learning. The model is a convolutional neural network, trained with a
      variant of Q-learning, whose input is raw pixels and whose output is a
      value function estimating future rewards. We apply our method to seven
      Atari 2600 games from the Arcade Learning Environment, with no adjustment
      of the architecture or learning algorithm. We find that it outperforms all
      previous approaches on six of the games and surpasses a human expert on
      three of them.
    title: Playing Atari with Deep Reinforcement Learning
    updated: 2013-12-19T16:00:08.000Z
    url: 'http://arxiv.org/abs/1312.5602v1'
    year: 2013
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Andriy Mnih
      - name: Charles Blundell
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1310.8499v2.pdf
    id: 'http://arxiv.org/abs/1310.8499v2'
    links:
      - href: 'http://arxiv.org/abs/1310.8499v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1310.8499v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1310.8499v2'
    published: 2013-10-31T13:47:30.000Z
    summary: >-
      We introduce a deep, generative autoencoder capable of learning
      hierarchies of distributed representations from data. Successive deep
      stochastic hidden layers are equipped with autoregressive connections,
      which enable the model to be sampled from quickly and exactly via
      ancestral sampling. We derive an efficient approximate parameter
      estimation method based on the minimum description length (MDL) principle,
      which can be seen as maximising a variational lower bound on the
      log-likelihood, with a feedforward neural network implementing approximate
      inference. We demonstrate state-of-the-art generative performance on a
      number of classic data sets: several UCI data sets, MNIST and Atari 2600
      games.
    title: Deep AutoRegressive Networks
    updated: 2014-05-20T16:22:43.000Z
    url: 'http://arxiv.org/abs/1310.8499v2'
    year: 2013
  - authors:
      - name: Yi Sun
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Juergen Schmidhuber
    categories:
      - cs.AI
    files:
      - 1209.5853v1.pdf
    id: 'http://arxiv.org/abs/1209.5853v1'
    links:
      - href: 'http://arxiv.org/abs/1209.5853v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1209.5853v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1209.5853v1'
    published: 2012-09-26T07:42:06.000Z
    summary: >-
      Efficient Natural Evolution Strategies (eNES) is a novel alternative to
      conventional evolutionary algorithms, using the natural gradient to adapt
      the mutation distribution. Unlike previous methods based on natural
      gradients, eNES uses a fast algorithm to calculate the inverse of the
      exact Fisher information matrix, thus increasing both robustness and
      performance of its evolution gradient estimation, even in higher
      dimensions. Additional novel aspects of eNES include optimal fitness
      baselines and importance mixing (a procedure for updating the population
      with very few fitness evaluations). The algorithm yields competitive
      results on both unimodal and multimodal benchmarks.
    title: Efficient Natural Evolution Strategies
    updated: 2012-09-26T07:42:06.000Z
    url: 'http://arxiv.org/abs/1209.5853v1'
    year: 2012
  - authors:
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Tobias Glasmachers
      - name: Yi Sun
      - name: Jürgen Schmidhuber
    categories:
      - stat.ML
      - cs.NE
    files:
      - 1106.4487v1.pdf
    id: 'http://arxiv.org/abs/1106.4487v1'
    links:
      - href: 'http://arxiv.org/abs/1106.4487v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1106.4487v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1106.4487v1'
    published: 2011-06-22T15:55:52.000Z
    summary: >-
      This paper presents Natural Evolution Strategies (NES), a recent family of
      algorithms that constitute a more principled approach to black-box
      optimization than established evolutionary algorithms. NES maintains a
      parameterized distribution on the set of solution candidates, and the
      natural gradient is used to update the distribution's parameters in the
      direction of higher expected fitness. We introduce a collection of
      techniques that address issues of convergence, robustness, sample
      complexity, computational complexity and sensitivity to hyperparameters.
      This paper explores a number of implementations of the NES family, ranging
      from general-purpose multi-variate normal distributions to heavy-tailed
      and separable distributions tailored towards global optimization and
      search in high dimensional spaces, respectively. Experimental results show
      best published performance on various standard benchmarks, as well as
      competitive performance on others.
    title: Natural Evolution Strategies
    updated: 2011-06-22T15:55:52.000Z
    url: 'http://arxiv.org/abs/1106.4487v1'
    year: 2011
  - authors:
      - name: Juergen Schmidhuber
      - name: Matteo Gagliolo
      - name: Daan Wierstra
      - name: Faustino Gomez
    categories:
      - cs.NE
      - F.1.1; I.2.6
    files:
      - 0512062v1.pdf
    id: 'http://arxiv.org/abs/cs/0512062v1'
    links:
      - href: 'http://arxiv.org/abs/cs/0512062v1'
        title: ''
      - href: 'http://arxiv.org/pdf/cs/0512062v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/cs/0512062v1'
    published: 2005-12-15T15:05:22.000Z
    summary: >-
      Traditional Support Vector Machines (SVMs) need pre-wired finite time
      windows to predict and classify time series. They do not have an internal
      state necessary to deal with sequences involving arbitrary long-term
      dependencies. Here we introduce a new class of recurrent, truly sequential
      SVM-like devices with internal adaptive states, trained by a novel method
      called EVOlution of systems with KErnel-based outputs (Evoke), an instance
      of the recent Evolino class of methods. Evoke evolves recurrent neural
      networks to detect and represent temporal dependencies while using
      quadratic programming/support vector regression to produce precise
      outputs. Evoke is the first SVM-based mechanism learning to classify a
      context-sensitive language. It also outperforms recent state-of-the-art
      gradient-based recurrent neural networks (RNNs) on various time series
      prediction tasks.
    title: Evolino for recurrent support vector machines
    updated: 2005-12-15T15:05:22.000Z
    url: 'http://arxiv.org/abs/cs/0512062v1'
    year: 2005
  - authors:
      - name: M. Botvinick
      - name: D. G. T. Barrett
      - name: P. Battaglia
      - name: N. de Freitas
      - name: D. Kumaran
      - name: J. Z Leibo
      - name: T. Lillicrap
      - name: J. Modayil
      - name: S. Mohamed
      - name: N. C. Rabinowitz
      - name: D. J. Rezende
      - name: A. Santoro
      - name: T. Schaul
      - name: C. Summerfield
      - name: G. Wayne
      - name: T. Weber
      - name: D. Wierstra
      - name: S. Legg
      - name: D. Hassabis
    categories:
      - cs.AI
    files:
      - 1711.08378v1.pdf
    id: 'http://arxiv.org/abs/1711.08378v1'
    links:
      - href: 'http://arxiv.org/abs/1711.08378v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1711.08378v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1711.08378v1'
    published: 2017-11-22T16:35:29.000Z
    summary: >-
      We agree with Lake and colleagues on their list of key ingredients for
      building humanlike intelligence, including the idea that model-based
      reasoning is essential. However, we favor an approach that centers on one
      additional ingredient: autonomy. In particular, we aim toward agents that
      can both build and exploit their own internal models, with minimal human
      hand-engineering. We believe an approach centered on autonomous learning
      has the greatest chance of success as we scale toward real-world
      complexity, tackling domains for which ready-made formal models are not
      available. Here we survey several important examples of the progress that
      has been made toward building autonomous agents with humanlike abilities,
      and highlight some outstanding challenges.
    title: >-
      Building Machines that Learn and Think for Themselves: Commentary on Lake
      et al., Behavioral and Brain Sciences, 2017
    updated: 2017-11-22T16:35:29.000Z
    url: 'http://arxiv.org/abs/1711.08378v1'
    year: 2017
  - authors:
      - name: Théophane Weber
      - name: Sébastien Racanière
      - name: David P. Reichert
      - name: Lars Buesing
      - name: Arthur Guez
      - name: Danilo Jimenez Rezende
      - name: Adria Puigdomènech Badia
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Yujia Li
      - name: Razvan Pascanu
      - name: Peter Battaglia
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
      - stat.ML
    files:
      - 1707.06203v1.pdf
    id: 'http://arxiv.org/abs/1707.06203v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06203v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06203v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06203v1'
    published: 2017-07-19T17:12:56.000Z
    summary: >-
      We introduce Imagination-Augmented Agents (I2As), a novel architecture for
      deep reinforcement learning combining model-free and model-based aspects.
      In contrast to most existing model-based reinforcement learning and
      planning methods, which prescribe how a model should be used to arrive at
      a policy, I2As learn to interpret predictions from a learned environment
      model to construct implicit plans in arbitrary ways, by using the
      predictions as additional context in deep policy networks. I2As show
      improved data efficiency, performance, and robustness to model
      misspecification compared to several baselines.
    title: Imagination-Augmented Agents for Deep Reinforcement Learning
    updated: 2017-07-19T17:12:56.000Z
    url: 'http://arxiv.org/abs/1707.06203v1'
    year: 2017
  - authors:
      - name: Razvan Pascanu
      - name: Yujia Li
      - name: Oriol Vinyals
      - name: Nicolas Heess
      - name: Lars Buesing
      - name: Sebastien Racanière
      - name: David Reichert
      - name: Théophane Weber
      - name: Daan Wierstra
      - name: Peter Battaglia
    categories:
      - cs.AI
      - cs.LG
      - cs.NE
      - stat.ML
    files:
      - 1707.06170v1.pdf
    id: 'http://arxiv.org/abs/1707.06170v1'
    links:
      - href: 'http://arxiv.org/abs/1707.06170v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1707.06170v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1707.06170v1'
    published: 2017-07-19T15:52:35.000Z
    summary: >-
      Conventional wisdom holds that model-based planning is a powerful approach
      to sequential decision-making. It is often very challenging in practice,
      however, because while a model can be used to evaluate a plan, it does not
      prescribe how to construct a plan. Here we introduce the
      "Imagination-based Planner", the first model-based, sequential
      decision-making agent that can learn to construct, evaluate, and execute
      plans. Before any action, it can perform a variable number of imagination
      steps, which involve proposing an imagined action and evaluating it with
      its model-based imagination. All imagined actions and outcomes are
      aggregated, iteratively, into a "plan context" which conditions future
      real and imagined actions. The agent can even decide how to imagine:
      testing out alternative imagined actions, chaining sequences of actions
      together, or building a more complex "imagination tree" by navigating
      flexibly among the previously imagined states using a learned policy. And
      our agent can learn to plan economically, jointly optimizing for external
      rewards and computational costs associated with using its imagination. We
      show that our architecture can learn to solve a challenging continuous
      control problem, and also learn elaborate planning strategies in a
      discrete maze-solving task. Our work opens a new direction toward learning
      the components of a model-based planning system and how to use them.
    title: Learning model-based planning from scratch
    updated: 2017-07-19T15:52:35.000Z
    url: 'http://arxiv.org/abs/1707.06170v1'
    year: 2017
  - authors:
      - name: Ivo Danihelka
      - name: Balaji Lakshminarayanan
      - name: Benigno Uria
      - name: Daan Wierstra
      - name: Peter Dayan
    categories:
      - cs.LG
    files:
      - 1705.05263v1.pdf
    id: 'http://arxiv.org/abs/1705.05263v1'
    links:
      - href: 'http://arxiv.org/abs/1705.05263v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1705.05263v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1705.05263v1'
    published: 2017-05-15T14:24:01.000Z
    summary: >-
      We train a generator by maximum likelihood and we also train the same
      generator architecture by Wasserstein GAN. We then compare the generated
      samples, exact log-probability densities and approximate Wasserstein
      distances. We show that an independent critic trained to approximate
      Wasserstein distance between the validation set and the generator
      distribution helps detect overfitting. Finally, we use ideas from the
      one-shot learning literature to develop a novel fast learning critic.
    title: Comparison of Maximum Likelihood and GAN-based training of Real NVPs
    updated: 2017-05-15T14:24:01.000Z
    url: 'http://arxiv.org/abs/1705.05263v1'
    year: 2017
  - authors:
      - name: Silvia Chiappa
      - name: Sébastien Racaniere
      - name: Daan Wierstra
      - name: Shakir Mohamed
    categories:
      - cs.AI
      - cs.LG
      - stat.ML
    files:
      - 1704.02254v2.pdf
    id: 'http://arxiv.org/abs/1704.02254v2'
    links:
      - href: 'http://arxiv.org/abs/1704.02254v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1704.02254v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1704.02254v2'
    published: 2017-04-07T14:53:54.000Z
    summary: >-
      Models that can simulate how environments change in response to actions
      can be used by agents to plan and act efficiently. We improve on previous
      environment simulators from high-dimensional pixel observations by
      introducing recurrent neural networks that are able to make temporally and
      spatially coherent predictions for hundreds of time-steps into the future.
      We present an in-depth analysis of the factors affecting performance,
      providing the most extensive attempt to advance the understanding of the
      properties of these models. We address the issue of computationally
      inefficiency with a model that does not need to generate a
      high-dimensional image at each time-step. We show that our approach can be
      used to improve exploration and is adaptable to many diverse environments,
      namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.
    title: Recurrent Environment Simulators
    updated: 2017-04-19T15:43:32.000Z
    url: 'http://arxiv.org/abs/1704.02254v2'
    year: 2017
  - authors:
      - name: Alexander Pritzel
      - name: Benigno Uria
      - name: Sriram Srinivasan
      - name: Adrià Puigdomènech
      - name: Oriol Vinyals
      - name: Demis Hassabis
      - name: Daan Wierstra
      - name: Charles Blundell
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1703.01988v1.pdf
    id: 'http://arxiv.org/abs/1703.01988v1'
    links:
      - href: 'http://arxiv.org/abs/1703.01988v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1703.01988v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1703.01988v1'
    published: 2017-03-06T17:23:27.000Z
    summary: >-
      Deep reinforcement learning methods attain super-human performance in a
      wide range of environments. Such methods are grossly inefficient, often
      taking orders of magnitudes more data than humans to achieve reasonable
      performance. We propose Neural Episodic Control: a deep reinforcement
      learning agent that is able to rapidly assimilate new experiences and act
      upon them. Our agent uses a semi-tabular representation of the value
      function: a buffer of past experience containing slowly changing state
      representations and rapidly updated estimates of the value function. We
      show across a wide range of environments that our agent learns
      significantly faster than other state-of-the-art, general purpose deep
      reinforcement learning agents.
    title: Neural Episodic Control
    updated: 2017-03-06T17:23:27.000Z
    url: 'http://arxiv.org/abs/1703.01988v1'
    year: 2017
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Charles Blundell
      - name: Yori Zwols
      - name: David Ha
      - name: Andrei A. Rusu
      - name: Alexander Pritzel
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.LG
    files:
      - 1701.08734v1.pdf
    id: 'http://arxiv.org/abs/1701.08734v1'
    links:
      - href: 'http://arxiv.org/abs/1701.08734v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1701.08734v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1701.08734v1'
    published: 2017-01-30T18:06:07.000Z
    summary: >-
      For artificial general intelligence (AGI) it would be efficient if
      multiple users trained the same giant neural network, permitting parameter
      reuse, without catastrophic forgetting. PathNet is a first step in this
      direction. It is a neural network algorithm that uses agents embedded in
      the neural network whose task is to discover which parts of the network to
      re-use for new tasks. Agents are pathways (views) through the network
      which determine the subset of parameters that are used and updated by the
      forwards and backwards passes of the backpropogation algorithm. During
      learning, a tournament selection genetic algorithm is used to select
      pathways through the neural network for replication and mutation. Pathway
      fitness is the performance of that pathway measured according to a cost
      function. We demonstrate successful transfer learning; fixing the
      parameters along a path learned on task A and re-evolving a new population
      of paths for task B, allows task B to be learned faster than it could be
      learned from scratch or after fine-tuning. Paths evolved on task B re-use
      parts of the optimal path evolved on task A. Positive transfer was
      demonstrated for binary MNIST, CIFAR, and SVHN supervised learning
      classification tasks, and a set of Atari and Labyrinth reinforcement
      learning tasks, suggesting PathNets have general applicability for neural
      network training. Finally, PathNet also significantly improves the
      robustness to hyperparameter choices of a parallel asynchronous
      reinforcement learning algorithm (A3C).
    title: 'PathNet: Evolution Channels Gradient Descent in Super Neural Networks'
    updated: 2017-01-30T18:06:07.000Z
    url: 'http://arxiv.org/abs/1701.08734v1'
    year: 2017
  - authors:
      - name: Karol Gregor
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.LG
      - cs.AI
    files:
      - 1611.07507v1.pdf
    id: 'http://arxiv.org/abs/1611.07507v1'
    links:
      - href: 'http://arxiv.org/abs/1611.07507v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1611.07507v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1611.07507v1'
    published: 2016-11-22T20:44:39.000Z
    summary: >-
      In this paper we introduce a new unsupervised reinforcement learning
      method for discovering the set of intrinsic options available to an agent.
      This set is learned by maximizing the number of different states an agent
      can reliably reach, as measured by the mutual information between the set
      of options and option termination states. To this end, we instantiate two
      policy gradient based algorithms, one that creates an explicit embedding
      space of options and one that represents options implicitly. The
      algorithms also provide an explicit measure of empowerment in a given
      state that can be used by an empowerment maximizing agent. The algorithm
      scales well with function approximation and we demonstrate the
      applicability of the algorithm on a range of tasks.
    title: Variational Intrinsic Control
    updated: 2016-11-22T20:44:39.000Z
    url: 'http://arxiv.org/abs/1611.07507v1'
    year: 2016
  - authors:
      - name: Charles Blundell
      - name: Benigno Uria
      - name: Alexander Pritzel
      - name: Yazhe Li
      - name: Avraham Ruderman
      - name: Joel Z Leibo
      - name: Jack Rae
      - name: Daan Wierstra
      - name: Demis Hassabis
    categories:
      - stat.ML
      - cs.LG
      - q-bio.NC
    files:
      - 1606.04460v1.pdf
    id: 'http://arxiv.org/abs/1606.04460v1'
    links:
      - href: 'http://arxiv.org/abs/1606.04460v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04460v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04460v1'
    published: 2016-06-14T17:03:46.000Z
    summary: >-
      State of the art deep reinforcement learning algorithms take many millions
      of interactions to attain human-level performance. Humans, on the other
      hand, can very quickly exploit highly rewarding nuances of an environment
      upon first discovery. In the brain, such rapid learning is thought to
      depend on the hippocampus and its capacity for episodic memory. Here we
      investigate whether a simple model of hippocampal episodic control can
      learn to solve difficult sequential decision-making tasks. We demonstrate
      that it not only attains a highly rewarding strategy significantly faster
      than state-of-the-art deep reinforcement learning algorithms, but also
      achieves a higher overall reward on some of the more challenging domains.
    title: Model-Free Episodic Control
    updated: 2016-06-14T17:03:46.000Z
    url: 'http://arxiv.org/abs/1606.04460v1'
    year: 2016
  - authors:
      - name: Oriol Vinyals
      - name: Charles Blundell
      - name: Timothy Lillicrap
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1606.04080v2.pdf
    id: 'http://arxiv.org/abs/1606.04080v2'
    links:
      - href: 'http://arxiv.org/abs/1606.04080v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.04080v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.04080v2'
    published: 2016-06-13T19:34:22.000Z
    summary: >-
      Learning from a few examples remains a key challenge in machine learning.
      Despite recent advances in important domains such as vision and language,
      the standard supervised deep learning paradigm does not offer a
      satisfactory solution for learning new concepts rapidly from little data.
      In this work, we employ ideas from metric learning based on deep neural
      features and from recent advances that augment neural networks with
      external memories. Our framework learns a network that maps a small
      labelled support set and an unlabelled example to its label, obviating the
      need for fine-tuning to adapt to new class types. We then define one-shot
      learning problems on vision (using Omniglot, ImageNet) and language tasks.
      Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2%
      and from 88.0% to 93.8% on Omniglot compared to competing approaches. We
      also demonstrate the usefulness of the same model on language modeling by
      introducing a one-shot task on the Penn Treebank.
    title: Matching Networks for One Shot Learning
    updated: 2017-12-29T17:45:19.000Z
    url: 'http://arxiv.org/abs/1606.04080v2'
    year: 2016
  - authors:
      - name: Chrisantha Fernando
      - name: Dylan Banarse
      - name: Malcolm Reynolds
      - name: Frederic Besse
      - name: David Pfau
      - name: Max Jaderberg
      - name: Marc Lanctot
      - name: Daan Wierstra
    categories:
      - cs.NE
      - cs.CV
      - cs.LG
    files:
      - 1606.02580v1.pdf
    id: 'http://arxiv.org/abs/1606.02580v1'
    links:
      - href: 'http://arxiv.org/abs/1606.02580v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1606.02580v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1606.02580v1'
    published: 2016-06-08T14:37:39.000Z
    summary: >-
      In this work we introduce a differentiable version of the Compositional
      Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the
      topology of a DPPN is evolved but the weights are learned. A Lamarckian
      algorithm, that combines evolution and learning, produces DPPNs to
      reconstruct an image. Our main result is that DPPNs can be evolved/trained
      to compress the weights of a denoising autoencoder from 157684 to roughly
      200 parameters, while achieving a reconstruction accuracy comparable to a
      fully connected network with more than two orders of magnitude more
      parameters. The regularization ability of the DPPN allows it to rediscover
      (approximate) convolutional network architectures embedded within a fully
      connected architecture. Such convolutional architectures are the current
      state of the art for many computer vision applications, so it is
      satisfying that DPPNs are capable of discovering this structure rather
      than having to build it in by design. DPPNs exhibit better generalization
      when tested on the Omniglot dataset after being trained on MNIST, than
      directly encoded fully connected autoencoders. DPPNs are therefore a new
      framework for integrating learning and evolution.
    title: 'Convolution by Evolution: Differentiable Pattern Producing Networks'
    updated: 2016-06-08T14:37:39.000Z
    url: 'http://arxiv.org/abs/1606.02580v1'
    year: 2016
  - authors:
      - name: Adam Santoro
      - name: Sergey Bartunov
      - name: Matthew Botvinick
      - name: Daan Wierstra
      - name: Timothy Lillicrap
    categories:
      - cs.LG
    files:
      - 1605.06065v1.pdf
    id: 'http://arxiv.org/abs/1605.06065v1'
    links:
      - href: 'http://arxiv.org/abs/1605.06065v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1605.06065v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1605.06065v1'
    published: 2016-05-19T17:44:51.000Z
    summary: >-
      Despite recent breakthroughs in the applications of deep neural networks,
      one setting that presents a persistent challenge is that of "one-shot
      learning." Traditional gradient-based networks require a lot of data to
      learn, often through extensive iterative training. When new data is
      encountered, the models must inefficiently relearn their parameters to
      adequately incorporate the new information without catastrophic
      interference. Architectures with augmented memory capacities, such as
      Neural Turing Machines (NTMs), offer the ability to quickly encode and
      retrieve new information, and hence can potentially obviate the downsides
      of conventional models. Here, we demonstrate the ability of a
      memory-augmented neural network to rapidly assimilate new data, and
      leverage this data to make accurate predictions after only a few samples.
      We also introduce a new method for accessing an external memory that
      focuses on memory content, unlike previous methods that additionally use
      memory location-based focusing mechanisms.
    title: One-shot Learning with Memory-Augmented Neural Networks
    updated: 2016-05-19T17:44:51.000Z
    url: 'http://arxiv.org/abs/1605.06065v1'
    year: 2016
  - authors:
      - name: Karol Gregor
      - name: Frederic Besse
      - name: Danilo Jimenez Rezende
      - name: Ivo Danihelka
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.CV
      - cs.LG
    files:
      - 1604.08772v1.pdf
    id: 'http://arxiv.org/abs/1604.08772v1'
    links:
      - href: 'http://arxiv.org/abs/1604.08772v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1604.08772v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1604.08772v1'
    published: 2016-04-29T11:02:52.000Z
    summary: >-
      We introduce a simple recurrent variational auto-encoder architecture that
      significantly improves image modeling. The system represents the
      state-of-the-art in latent variable models for both the ImageNet and
      Omniglot datasets. We show that it naturally separates global conceptual
      information from lower level details, thus addressing one of the
      fundamentally desired properties of unsupervised learning. Furthermore,
      the possibility of restricting ourselves to storing only global
      information about an image allows us to achieve high quality 'conceptual
      compression'.
    title: Towards Conceptual Compression
    updated: 2016-04-29T11:02:52.000Z
    url: 'http://arxiv.org/abs/1604.08772v1'
    year: 2016
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Ivo Danihelka
      - name: Karol Gregor
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
    files:
      - 1603.05106v2.pdf
    id: 'http://arxiv.org/abs/1603.05106v2'
    links:
      - href: 'http://arxiv.org/abs/1603.05106v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1603.05106v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1603.05106v2'
    published: 2016-03-16T14:10:00.000Z
    summary: >-
      Humans have an impressive ability to reason about new concepts and
      experiences from just a single example. In particular, humans have an
      ability for one-shot generalization: an ability to encounter a new
      concept, understand its structure, and then be able to generate compelling
      alternative variations of the concept. We develop machine learning systems
      with this important capacity by developing new deep generative models,
      models that combine the representational power of deep learning with the
      inferential power of Bayesian reasoning. We develop a class of sequential
      generative models that are built on the principles of feedback and
      attention. These two characteristics lead to generative models that are
      among the state-of-the art in density estimation and image generation. We
      demonstrate the one-shot generalization ability of our models using three
      tasks: unconditional sampling, generating new exemplars of a given
      concept, and generating new exemplars of a family of concepts. In all
      cases our models are able to generate compelling and diverse
      samples---having seen new examples just once---providing an important
      class of general-purpose models for one-shot machine learning.
    title: One-Shot Generalization in Deep Generative Models
    updated: 2016-05-25T12:57:19.000Z
    url: 'http://arxiv.org/abs/1603.05106v2'
    year: 2016
  - authors:
      - name: Timothy P. Lillicrap
      - name: Jonathan J. Hunt
      - name: Alexander Pritzel
      - name: Nicolas Heess
      - name: Tom Erez
      - name: Yuval Tassa
      - name: David Silver
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1509.02971v5.pdf
    id: 'http://arxiv.org/abs/1509.02971v5'
    links:
      - href: 'http://arxiv.org/abs/1509.02971v5'
        title: ''
      - href: 'http://arxiv.org/pdf/1509.02971v5'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1509.02971v5'
    published: 2015-09-09T23:01:36.000Z
    summary: >-
      We adapt the ideas underlying the success of Deep Q-Learning to the
      continuous action domain. We present an actor-critic, model-free algorithm
      based on the deterministic policy gradient that can operate over
      continuous action spaces. Using the same learning algorithm, network
      architecture and hyper-parameters, our algorithm robustly solves more than
      20 simulated physics tasks, including classic problems such as cartpole
      swing-up, dexterous manipulation, legged locomotion and car driving. Our
      algorithm is able to find policies whose performance is competitive with
      those found by a planning algorithm with full access to the dynamics of
      the domain and its derivatives. We further demonstrate that for many of
      the tasks the algorithm can learn policies end-to-end: directly from raw
      pixel inputs.
    title: Continuous control with deep reinforcement learning
    updated: 2016-02-29T18:45:53.000Z
    url: 'http://arxiv.org/abs/1509.02971v5'
    year: 2015
  - authors:
      - name: Charles Blundell
      - name: Julien Cornebise
      - name: Koray Kavukcuoglu
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.LG
    files:
      - 1505.05424v2.pdf
    id: 'http://arxiv.org/abs/1505.05424v2'
    links:
      - href: 'http://arxiv.org/abs/1505.05424v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1505.05424v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1505.05424v2'
    published: 2015-05-20T15:39:48.000Z
    summary: >-
      We introduce a new, efficient, principled and backpropagation-compatible
      algorithm for learning a probability distribution on the weights of a
      neural network, called Bayes by Backprop. It regularises the weights by
      minimising a compression cost, known as the variational free energy or the
      expected lower bound on the marginal likelihood. We show that this
      principled kind of regularisation yields comparable performance to dropout
      on MNIST classification. We then demonstrate how the learnt uncertainty in
      the weights can be used to improve generalisation in non-linear regression
      problems, and how this weight uncertainty can be used to drive the
      exploration-exploitation trade-off in reinforcement learning.
    title: Weight Uncertainty in Neural Networks
    updated: 2015-05-21T14:07:23.000Z
    url: 'http://arxiv.org/abs/1505.05424v2'
    year: 2015
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Alex Graves
      - name: Danilo Jimenez Rezende
      - name: Daan Wierstra
    categories:
      - cs.CV
      - cs.LG
      - cs.NE
    files:
      - 1502.04623v2.pdf
    id: 'http://arxiv.org/abs/1502.04623v2'
    links:
      - href: 'http://arxiv.org/abs/1502.04623v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1502.04623v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1502.04623v2'
    published: 2015-02-16T16:48:56.000Z
    summary: >-
      This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural
      network architecture for image generation. DRAW networks combine a novel
      spatial attention mechanism that mimics the foveation of the human eye,
      with a sequential variational auto-encoding framework that allows for the
      iterative construction of complex images. The system substantially
      improves on the state of the art for generative models on MNIST, and, when
      trained on the Street View House Numbers dataset, it generates images that
      cannot be distinguished from real data with the naked eye.
    title: 'DRAW: A Recurrent Neural Network For Image Generation'
    updated: 2015-05-20T15:29:42.000Z
    url: 'http://arxiv.org/abs/1502.04623v2'
    year: 2015
  - authors:
      - name: Danilo Jimenez Rezende
      - name: Shakir Mohamed
      - name: Daan Wierstra
    categories:
      - stat.ML
      - cs.AI
      - cs.LG
      - stat.CO
      - stat.ME
    files:
      - 1401.4082v3.pdf
    id: 'http://arxiv.org/abs/1401.4082v3'
    links:
      - href: 'http://arxiv.org/abs/1401.4082v3'
        title: ''
      - href: 'http://arxiv.org/pdf/1401.4082v3'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1401.4082v3'
    published: 2014-01-16T16:33:23.000Z
    summary: >-
      We marry ideas from deep neural networks and approximate Bayesian
      inference to derive a generalised class of deep, directed generative
      models, endowed with a new algorithm for scalable inference and learning.
      Our algorithm introduces a recognition model to represent approximate
      posterior distributions, and that acts as a stochastic encoder of the
      data. We develop stochastic back-propagation -- rules for back-propagation
      through stochastic variables -- and use this to develop an algorithm that
      allows for joint optimisation of the parameters of both the generative and
      recognition model. We demonstrate on several real-world data sets that the
      model generates realistic samples, provides accurate imputations of
      missing data and is a useful tool for high-dimensional data visualisation.
    title: >-
      Stochastic Backpropagation and Approximate Inference in Deep Generative
      Models
    updated: 2014-05-30T10:00:36.000Z
    url: 'http://arxiv.org/abs/1401.4082v3'
    year: 2014
  - authors:
      - name: Volodymyr Mnih
      - name: Koray Kavukcuoglu
      - name: David Silver
      - name: Alex Graves
      - name: Ioannis Antonoglou
      - name: Daan Wierstra
      - name: Martin Riedmiller
    categories:
      - cs.LG
    files:
      - 1312.5602v1.pdf
    id: 'http://arxiv.org/abs/1312.5602v1'
    links:
      - href: 'http://arxiv.org/abs/1312.5602v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1312.5602v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1312.5602v1'
    published: 2013-12-19T16:00:08.000Z
    summary: >-
      We present the first deep learning model to successfully learn control
      policies directly from high-dimensional sensory input using reinforcement
      learning. The model is a convolutional neural network, trained with a
      variant of Q-learning, whose input is raw pixels and whose output is a
      value function estimating future rewards. We apply our method to seven
      Atari 2600 games from the Arcade Learning Environment, with no adjustment
      of the architecture or learning algorithm. We find that it outperforms all
      previous approaches on six of the games and surpasses a human expert on
      three of them.
    title: Playing Atari with Deep Reinforcement Learning
    updated: 2013-12-19T16:00:08.000Z
    url: 'http://arxiv.org/abs/1312.5602v1'
    year: 2013
  - authors:
      - name: Karol Gregor
      - name: Ivo Danihelka
      - name: Andriy Mnih
      - name: Charles Blundell
      - name: Daan Wierstra
    categories:
      - cs.LG
      - stat.ML
    files:
      - 1310.8499v2.pdf
    id: 'http://arxiv.org/abs/1310.8499v2'
    links:
      - href: 'http://arxiv.org/abs/1310.8499v2'
        title: ''
      - href: 'http://arxiv.org/pdf/1310.8499v2'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1310.8499v2'
    published: 2013-10-31T13:47:30.000Z
    summary: >-
      We introduce a deep, generative autoencoder capable of learning
      hierarchies of distributed representations from data. Successive deep
      stochastic hidden layers are equipped with autoregressive connections,
      which enable the model to be sampled from quickly and exactly via
      ancestral sampling. We derive an efficient approximate parameter
      estimation method based on the minimum description length (MDL) principle,
      which can be seen as maximising a variational lower bound on the
      log-likelihood, with a feedforward neural network implementing approximate
      inference. We demonstrate state-of-the-art generative performance on a
      number of classic data sets: several UCI data sets, MNIST and Atari 2600
      games.
    title: Deep AutoRegressive Networks
    updated: 2014-05-20T16:22:43.000Z
    url: 'http://arxiv.org/abs/1310.8499v2'
    year: 2013
  - authors:
      - name: Yi Sun
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Juergen Schmidhuber
    categories:
      - cs.AI
    files:
      - 1209.5853v1.pdf
    id: 'http://arxiv.org/abs/1209.5853v1'
    links:
      - href: 'http://arxiv.org/abs/1209.5853v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1209.5853v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1209.5853v1'
    published: 2012-09-26T07:42:06.000Z
    summary: >-
      Efficient Natural Evolution Strategies (eNES) is a novel alternative to
      conventional evolutionary algorithms, using the natural gradient to adapt
      the mutation distribution. Unlike previous methods based on natural
      gradients, eNES uses a fast algorithm to calculate the inverse of the
      exact Fisher information matrix, thus increasing both robustness and
      performance of its evolution gradient estimation, even in higher
      dimensions. Additional novel aspects of eNES include optimal fitness
      baselines and importance mixing (a procedure for updating the population
      with very few fitness evaluations). The algorithm yields competitive
      results on both unimodal and multimodal benchmarks.
    title: Efficient Natural Evolution Strategies
    updated: 2012-09-26T07:42:06.000Z
    url: 'http://arxiv.org/abs/1209.5853v1'
    year: 2012
  - authors:
      - name: Daan Wierstra
      - name: Tom Schaul
      - name: Tobias Glasmachers
      - name: Yi Sun
      - name: Jürgen Schmidhuber
    categories:
      - stat.ML
      - cs.NE
    files:
      - 1106.4487v1.pdf
    id: 'http://arxiv.org/abs/1106.4487v1'
    links:
      - href: 'http://arxiv.org/abs/1106.4487v1'
        title: ''
      - href: 'http://arxiv.org/pdf/1106.4487v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/1106.4487v1'
    published: 2011-06-22T15:55:52.000Z
    summary: >-
      This paper presents Natural Evolution Strategies (NES), a recent family of
      algorithms that constitute a more principled approach to black-box
      optimization than established evolutionary algorithms. NES maintains a
      parameterized distribution on the set of solution candidates, and the
      natural gradient is used to update the distribution's parameters in the
      direction of higher expected fitness. We introduce a collection of
      techniques that address issues of convergence, robustness, sample
      complexity, computational complexity and sensitivity to hyperparameters.
      This paper explores a number of implementations of the NES family, ranging
      from general-purpose multi-variate normal distributions to heavy-tailed
      and separable distributions tailored towards global optimization and
      search in high dimensional spaces, respectively. Experimental results show
      best published performance on various standard benchmarks, as well as
      competitive performance on others.
    title: Natural Evolution Strategies
    updated: 2011-06-22T15:55:52.000Z
    url: 'http://arxiv.org/abs/1106.4487v1'
    year: 2011
  - authors:
      - name: Juergen Schmidhuber
      - name: Matteo Gagliolo
      - name: Daan Wierstra
      - name: Faustino Gomez
    categories:
      - cs.NE
      - F.1.1; I.2.6
    files:
      - 0512062v1.pdf
    id: 'http://arxiv.org/abs/cs/0512062v1'
    links:
      - href: 'http://arxiv.org/abs/cs/0512062v1'
        title: ''
      - href: 'http://arxiv.org/pdf/cs/0512062v1'
        title: pdf
    pdfUrl: 'http://arxiv.org/pdf/cs/0512062v1'
    published: 2005-12-15T15:05:22.000Z
    summary: >-
      Traditional Support Vector Machines (SVMs) need pre-wired finite time
      windows to predict and classify time series. They do not have an internal
      state necessary to deal with sequences involving arbitrary long-term
      dependencies. Here we introduce a new class of recurrent, truly sequential
      SVM-like devices with internal adaptive states, trained by a novel method
      called EVOlution of systems with KErnel-based outputs (Evoke), an instance
      of the recent Evolino class of methods. Evoke evolves recurrent neural
      networks to detect and represent temporal dependencies while using
      quadratic programming/support vector regression to produce precise
      outputs. Evoke is the first SVM-based mechanism learning to classify a
      context-sensitive language. It also outperforms recent state-of-the-art
      gradient-based recurrent neural networks (RNNs) on various time series
      prediction tasks.
    title: Evolino for recurrent support vector machines
    updated: 2005-12-15T15:05:22.000Z
    url: 'http://arxiv.org/abs/cs/0512062v1'
    year: 2005
search:
  limit: 15
  open: true
  source: arxiv
